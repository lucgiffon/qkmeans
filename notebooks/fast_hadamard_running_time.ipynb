{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to implement a fast transform as a product of sparse matrices and check whether this implementation is actually faster than the naive dense matrix/vector product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** versioning a notebook is not trivial and you should be very careful since every committed version cannot be deleted!\n",
    "\n",
    "A notebook file contains both:\n",
    "\n",
    "* the source code (cells): it is stored efficiently in text format, and does not change when runnning the notebook.\n",
    "* and, when computed, the results: they are stored in binary format, not efficiently, changing every time the notebook is run, even if no change is made in the source code.\n",
    "\n",
    "So please be very careful:\n",
    "\n",
    "* never commit a notebook that has been run and where results are displayed;\n",
    "* before committing changes in a notebook, just execute `Restard and Clear Output` in the `Kernel` menu: this will delete all the binary output and only the source code will be committed; another option is to run `git checkout my_notebook.ipynb` in order to revert to the commited version and to ignore the delete the current version (assuming that there is no change you want to commit).\n",
    "* if you are used to committing all the files that have been changed (e.g. if you use a graphical interface for git or if you use option `git commit -a`), then you may commit binary contents of a notebook by mistake; in this case, you may copy the notebook and run the copied non-versionned file (not using the committed version);\n",
    "* if you have any doubt or question, ask before any commit.\n",
    "\n",
    "Note that we may find a better solution ([like this one](https://stackoverflow.com/questions/18734739/using-ipython-notebooks-under-version-control/20844506#20844506)) but this is something discussed several times with Florent and Denis and that is not trivial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import hadamard as sp_hadamard_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadamard transform of a vector $v$ in dimension $n$ can be computed as the product $ F^{\\log_2 n} v$ where $F$ is a sparse $n \\times n$ matrix. The following function builds the $F$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_hadamard_factor(n):\n",
    "    \"\"\"\n",
    "    Build the sparse matrix used to compute the fast Hadamard transform.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Size of the transform\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    csr_matrix\n",
    "        Sparse n-by-n matrix\n",
    "    \"\"\"\n",
    "    F = np.zeros((n, n), dtype=int)\n",
    "    F[range(0, n, 2), range(0, n//2)] = 1\n",
    "    F[range(1, n, 2), range(0, n//2)] = 1\n",
    "    F[range(0, n, 2), range(n//2, n)] = 1\n",
    "    F[range(1, n, 2), range(n//2, n)] = -1\n",
    "    return csr_matrix(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_hadamard_matrix(n):\n",
    "    \"\"\"\n",
    "    Build the dense matrix used to compute the (slow) Hadamard transform.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Size of the transform\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nd-array\n",
    "        Hadamard n-by-n matrix\n",
    "    \"\"\"\n",
    "    F = build_hadamard_factor(n).toarray()\n",
    "    H = np.linalg.matrix_power(F, int(np.log2(n)))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the function and check that our implementation of Hadamard matrices gives the correct result (i.e., the same result as function `scipy.linalg.hadamard`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(build_hadamard_matrix(32))\n",
    "print(np.all(sp_hadamard_matrix(32) == build_hadamard_matrix(32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the main function that implements the fast Hadamard transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_hadamard(v, F=None):\n",
    "    \"\"\"\n",
    "    Apply fast Hadamard transform using the product of sparse matrices.\n",
    "    \n",
    "    The Hadamard transform of a vector $v$ in dimension $n$ can be computed as the product $ F^{\\log_2 n} v$\n",
    "    where $F$ is a sparse $n \\times n$ matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    v : nd-array [n]\n",
    "        Vector to be transformed\n",
    "    \n",
    "    F : nd-array [n, n], optional\n",
    "        Sparse factor to compute the Hadamard transform.\n",
    "        If `None`, the matrix is computed using function `build_hadamard_factor`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nd-array [n]\n",
    "        Vector resulting form the Hadamard transform of v.\n",
    "    \"\"\"\n",
    "    n = v.size\n",
    "    if F is None:\n",
    "        F = build_hadamard_factor(n)\n",
    "    for i in range(int(np.log2(n))):\n",
    "        v = F.dot(v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that the fast transform give the same result as the matrix-vector product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 32\n",
    "v = np.random.randn(n)\n",
    "np.max(np.abs(apply_hadamard(v) - build_hadamard_matrix(n) @ v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can measure and compare the running times of the fast transform vs. the product between the dense Hadamard matrix and a data vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_sparse_matrix = []\n",
    "t_full_matrix = []\n",
    "res = []\n",
    "p_values = np.arange(16)\n",
    "n_trials = 50\n",
    "for n in 2**p_values:\n",
    "    print(n)\n",
    "    v = np.random.randn(n)\n",
    "    \n",
    "    F = build_hadamard_factor(n)\n",
    "    cum_time = 0\n",
    "    for _ in range(n_trials):\n",
    "        t0 = perf_counter()\n",
    "        v_sparse = apply_hadamard(v, F)\n",
    "        cum_time += perf_counter() - t0\n",
    "    t_sparse_matrix.append(cum_time / n_trials)\n",
    "    \n",
    "    # Beyond some size limit, skip the computation of the slow method\n",
    "    if n <= 1024:\n",
    "        H = build_hadamard_matrix(n)\n",
    "        cum_time = 0\n",
    "        for _ in range(n_trials):\n",
    "            t0 = perf_counter()\n",
    "            v_full = H @ v\n",
    "            cum_time += perf_counter() - t0\n",
    "        t_full_matrix.append(cum_time / n_trials)\n",
    "        res.append(np.max(np.abs(v_full - v_sparse)))\n",
    "    else:\n",
    "        t_full_matrix.append(np.nan)\n",
    "        res.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(2**p_values, t_full_matrix, label='full')\n",
    "plt.semilogy(2**p_values, t_sparse_matrix, label='sparse')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('Average running time ({} trials)'.format(n_trials))\n",
    "plt.xlabel('Transform length')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(res)\n",
    "plt.ylabel('residual error ($\\|.\\|_\\infty$)')\n",
    "plt.xlabel('Transform length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
