\documentclass{article}
\pdfoutput=1
\usepackage[preprint]{neurips_2019}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
 %\usepackage{neurips_2019}

% OUR ADDITIONS IN PREAMBLE
%====================================================================================
\input{preamble.tex}
%====================================================================================


\newtheorem*{remark}{Remark}
\newtheorem*{proposition}{Proposition}
\newcommand{\diag}{\text{diag}}
\newcommand{\indicator}{\mathds{1}}


\title{QuicK-means: Acceleration of K-means by learning a fast transform}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Luc Giffon\\%\thanks{to be completed} \\
Aix Marseille Univ, CNRS, LIS, Marseille, France\\
%Aix Marseille Université, Université de Toulon, CNRS, LIS, Marseille, France\\
%  Address\\
%  \texttt{email} \\
  % examples of more authors
   \And
   Valentin Emiya\\
Aix Marseille Univ, CNRS, LIS, Marseille, France\\
%Aix Marseille Université, Université de Toulon, CNRS, LIS, Marseille, France\\
%  Address\\
%  \texttt{email} \\
   \And
   Liva Ralaivola\\
  Criteo\\
  Aix Marseille Univ, CNRS, LIS, Marseille, France\\
%Aix Marseille Université, Université de Toulon, CNRS, LIS, Marseille, France\\
%  \texttt{email} \\
   \And
   Hachem Kadri\\
Aix Marseille Univ, CNRS, LIS, Marseille, France\\
%Aix Marseille Université, Université de Toulon, CNRS, LIS, Marseille, France\\
%  Address\\
  %\texttt{email} \\
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
	
	\kmeans -- and the celebrated Lloyd algorithm -- is more than the clustering method it was originally designed to be. 
	It has indeed proven pivotal to help increase the speed of many machine learning and data analysis techniques such as indexing, nearest-neighbor search and prediction, data compression; its beneficial use has been shown to carry over to the acceleration of kernel machines (when using the Nyström method). 
	Here, we propose a fast extension of \kmeans, dubbed \texttt{QuicK-means}, that rests on the idea of expressing the matrix of the $\nclusters$ centroids as a product of sparse matrices, a feat made possible by recent results devoted to find approximations of matrices as a product of sparse factors. Using such a decomposition squashes the complexity of the matrix-vector product between the factorized $\nclusters \times \datadim$ centroid matrix $\mathbf{U}$ and any vector from $\mathcal{O}(\nclusters \datadim)$ to $\mathcal{O}(A \log A+B)$, with $A=\min (\nclusters, \datadim)$ and $B=\max (\nclusters, \datadim)$, where $\datadim$ is the dimension of the training data. This drastic computational saving has a direct impact in the assignment process of a point to a cluster, meaning that it is not only tangible at prediction time, but also at training time, provided the factorization procedure is performed during Lloyd's algorithm. We precisely show that resorting to a factorization step at each iteration does not impair the convergence of the optimization scheme and that, depending on the context, it may entail a reduction of the training time. Finally, we provide discussions and numerical simulations that show the versatility of our computationally-efficient  \texttt{QuicK-means} algorithm. 
	
%K-means -- and the celebrated Lloyd algorithm -- is more than the clustering method it was 
%originally designed to be. It has indeed proven pivotal to help increase the speed of
%many machine learning and data analysis techniques such as indexing, nearest-neighbor 
%search and prediction, data compression, Radial Basis Function networks;
% its beneficial use has been shown to carry over to the acceleration of kernel machines 
% (when using the Nyström method). Here, we propose a fast extension of K-means, dubbed QuicK-means, 
% that rests on
%the idea of expressing the matrix of the $K$ centroids
%as a product of sparse matrix, a feat made possible by recent results devoted 
%to find approximations of matrices as a product of sparse factors. Using such a 
%decomposition squashes the complexity of the matrix-vector product between
%the factorized $K \times D$ centroid matrix $\mathbf{U}$ and any vector from
%$\mathcal{O}(KD)$ to $\mathcal{O}(P+Q \log Q)$, with $Q=\min (K, D)$ and $P=\max (K, D)$,
%where $D$ is the dimension of the training data. This drastic computational saving
%has a direct impact in the assignment process of a point to a cluster, 
%meaning that it is not only tangible at prediction time, but also at training time,
%provided the factorization procedure is performed during Lloyd's algorithm.
%We precisely show that resorting to a factorization step at each iteration does not 
%impair the convergence of the optimization scheme and that, depending on the context, 
%it may entail a reduction of the training time. Finally, we provide discussions and numerical
% simulations that show the versatility of our computationally-efficient QuicK-means algorithm.



%
%Beyond its popularity for clustering, the K-means algorithm is a pivotal procedure for other core machine learning and data analysis techniques such as indexing, nearest-neightbors prediction, as well as for more specific approaches like the Nyström approximation for kernel machines.
%
%In this paper, we propose the Q-means algorithm, an accelerated version of $K$-means that stems from recent advances in optimization to learn the centroid matrix as a product of sparse matrices.
%This decomposition provides a structure similar to that of fast transforms (e.g., Fourier, Hadamard) in order to benefit from its computationnal efficiency while being adapted to the training data.
%Indeed, the complexity of the matrix-vector product between the factorized $K \times D$ matrix $\mathbf{U}$ and any vector is lowered from $\mathcal{O}(KD)$ to $\mathcal{O}(P+Q \log Q)$, with $Q=\min (K, D)$ and $P=\max (K, D)$.
%This dramatic acceleration is beneficial whenever a point is assigned to a cluster, i.e., at prediction time and in the assignation step at learning time.
%In addition, we show that the computational overhead due to the decomposition procedure does not penalize the computational cost of the learning stage, 
%which may be faster than the traditionnal Lloyd algorithm depending on the context.
%
%Finally, we provide discussions and numerical experiments that show the versatility of the proposed computationally-efficient Q-means algorithm.

%\addVE{Remarque: on ne mentionne pas la qualité de l'approximation qu'on obtient en remplaçant K-means par Q-means?!} \addLG{je crois qu'on peut assez peu s'exprimer à ce sujet sans borne...}
%
%\addVE{Remarque sur la complexité $\mathcal{O}(p+q \log q)$: on a $\log q$ facteurs dont un de taille $p\times q$ (ou l'inverse) a $\mathcal{O}(p)$ valeurs non-nulles et tous les autres de taille $q \times q$ ont $\mathcal{O}(q)$ valeurs non-nulles.}
\end{abstract}

%\addVE{Remarque: on ne mentionne pas la qualité de l'approximation qu'on obtient en remplaçant K-means par Q-means?!} \addLG{je crois qu'on peut assez peu s'exprimer à ce sujet sans borne...}

%\addVE{Remarque sur la complexité $\mathcal{O}(p+q \log q)$: on a $\log q$ facteurs dont un de taille $p\times q$ (ou l'inverse) a $\mathcal{O}(p)$ valeurs non-nulles et tous les autres de taille $q \times q$ ont $\mathcal{O}(q)$ valeurs non-nulles.}




% OUR PAPER
%====================================================================================
\input{introduction.tex}
\input{background.tex}
\input{contributions.tex}
\input{applications.tex}
\input{conclusion.tex}
%====================================================================================


\bibliographystyle{plain}
\bibliography{qmeans}

\appendix
\input{app_palm4msa}

\end{document}
