%!TEX root=neurips2019_qmeans.tex

\section{QuicK-means}
\label{sec:contribution}
\todo[inline]{Unify the notation and give it some aesthetics: $X_i$ for a vector is
ugly and inconsistent with the table of notation, $\rvx_i$ should be preferred.}

We here introduce our main contribution, QuicK-means (abbreviated by QK-means), 
show its convergence property and analyze its computational complexity.

\subsection{QK-means: Encoding Centroids as Products of Sparse Matrices}

QuicK-means is an extension of the K-means algorithm in which the matrix of centroids $\rmU$
is sought for as a product $\prod_{j=0}^Q\rmS_j$ of sparse matrices $\rmS_j$.
% $\mathcal{S}_j: j = 0 \ldots Q$. 
Doing so will allow us to cope with the computational bulk imposed by the product $\rmU\rvx$
(cf.~\eqref{eq:assignment_problem_kmeans}) at the core of the cluster assignement process.

Building upon~\eqref{eq:kmean_problem} and~\eqref{eq:problem_gribon} the optimisation problem 
of QK-means may be written as follows:
%
\begin{align}
\label{eq:qmean_problem}
 \min_{\rmU\in\{\prod_{q=1}^Q\rmS_j\}, \rvt} & g\left(\rmU, \rvt\right)
    \doteq \sum_{k=1}^{K}\sum_{j: t_j = k} \left\|\rvx_j -\rvu_k\right\|^2 + \sum_{q=0}^{Q} \delta_q(\rmS_q)
\end{align}
%
Here, 
as before, the $\delta_q$'s enforce the sparsity of the $\rmS_q$'s: $\delta_q(\emS_q)=0$ if $\rmS_q$ fulfills the sparsity constraints
and $+\infty$ otherwise.

\todo[inline]{Talk about the sizes of the matrices in the optimization problem?}

This problem can be solved using the QKmeans algorithm, given in Algorithm~\ref{algo:qmeans},
which proceeds in a similar way as Lloyd's algorithm with the difference being the 
recourse to a sparse factor decomposition of the matrix of centroids at
each re-estimation step. More precisely, at each iteration $\tau$, 
QKmeans alternates between two 
optimization procedures i) one which computes the assignement vector $\rvt^{(\tau)}$ 
of each training point to a cluster $k$, the matrix of centroids $\rmU^{(\tau-1)}$ being
fixed (and coming from the previous iteration) and ii) the reestimation of the centroids
 $\rmU^{(\tau)}$ given the new assignment $\rvt^{(\tau)}$. A synthetic view of the algorithm
 is:
\begin{subequations}
\label{eq:qmeans}
\begin{align}
\rvt^{(\tau)} &= \argmin_{\rvt\in\llbracket K\rrbracket^n}\sum_{j=1}^n\|\rvx_j-\rvu_{t_j}^{(\tau-1)}\|_{2}^2\label{eq:assignment}\\
\rmU^{(\tau)} &= \argmin_{\rmU\in\{\prod_{j=0}^Q\rmS_j\}}\|\rmD^{(\tau)}\hat{\rmX}^{(\tau)}-U\|_F^2+\sum_{q=0}^{Q} \delta_q(\rmS_q)\label{eq:reestimation}
\end{align}
\end{subequations}
where $\rmD^{(\tau)}=\diag\left({n_1^{(\tau)}},\ldots,{n_K^{(\tau)}}\right)^{1/2}$ is the square
root of the diagonal matrix made of the sizes of each cluster, and $\hat{\rmX}^{(\tau)}\doteq \left[\hat{\rvx}^{(\tau)}_1\cdots\hat{\rvx}^{(\tau)}_K\right]$
is the $D\times K$ matrix that has the barycenter $\hat{\rvx}^{(\tau)}_k$ of cluster $k$ as
its $k$th column, i.e.
\begin{align}
	\hat{\rvx}^{(\tau)}_k \doteq \frac{1}{n_k^{(\tau)}}\sum_{j:t_j^{(\tau)}=k}\rvx_j,
\end{align}

\todo[inline]{Find a compact way/notation to i) denote the set of products
of sparse factors  and ii) the result of Palm4MSA. And then, unify the notation.}

%As we show just below, QKmeans is guaranteed to converge.



\begin{remark}[Assignment/Re-estimation trade-off.]
A strategy to tackle this problem would be to first run the vanilla K-means algorithm,
 obtain the matrix of centroids $U$ and then encode $U$ as a product of sparse matrices
 using HierarchicalPalm4LED. This would however prevent us from taking advantage of 
 the expected low complexity product that plays a role in the assignement step of 
 the procedure.
\end{remark}

\todo[inline]{At some point, talk about the trade-off that we are playing with
regarding the cost of the assignment and the cost of the re-estimation procedure.}


As we will prove, the following result stands:
\begin{proposition}
Iterations over~\eqref{eq:assignment} and~\eqref{eq:reestimation} (and thus, QKmeans, Algorithm~\ref{algo:qmeans}) converge. 
\end{proposition}
\begin{proof}
	The proof rests upon a rewriting of the objective function $g$ of~\eqref{eq:qmean_problem}. Given $\rmU$ and $\rvt$, using $\hat{\rvx}_k$ to denote
	the mean of cluster $k$ as determined by $\rvt$, we have, for any $k$
\begin{align*}
	\sum_{j: t_j = k} &\left\|\rvx_j -\rvu_k\right\|^2
	=\sum_{j: t_j = k} \left\|\rvx_j -\hat{\rvx}_k+\hat{\rvx}_k-\rvu_k\right\|^2\\
	& = \sum_{j: t_j = k}\left(\|\rvx_j-\hat{\rvx}_k\|^2+\|\hat{\rvx}_k-\rvu_k\|^2-2\langle\rvx_j-\hat{\rvx}_k,\hat{\rvx}_k-\rvu_k\rangle\right)\notag\\
	& = \sum_{j: t_j = k}\|\rvx_j-\hat{\rvx}_k\|^2+n_k\|\hat{\rvx}_k-\rvu_k\|^2- 2\sum_{k=1}^{K}\left\langle\underbrace{\sum_{j: t_j = k}(\rvx_j-\hat{\rvx}_k)}_{=0},\hat{\rvx}_k-\rvu_k\right\rangle\notag\\
%	&= \sum_{k=1}^{K}\left[\sum_{j: \rvt_j = k}\|\rvx_j-\hat{\rvx}_k\|^2+n_k\|\hat{\rvx}_k-\rvu_k\|^2-\langle n_k\hat{\rvx}_k-n_k\hat{\rvx}_k,\hat{\rvx}_k-\rvu_k\rangle\right]\\
	&= \sum_{j: t_j = k}\|\rvx_j-\hat{\rvx}_k\|^2 + n_k\|\hat{\rvx}_k-\rvu_k\|^2
\end{align*}
whence, with a slight abuse of notation that discards
sparsity enforcing term, $g$ might be rewritten as:
\begin{align}
	g(\rmU,\rvt)&=\sum_{k=1}^K\sum_{j: t_j = k}\left\|\rvx_j -\rvu_k\right\|^2=\sum_{k=1}^K\sum_{j: t_j = k}\|\rvx_j-\hat{\rvx}_k\|^2 + \sum_{k=1}^Kn_k\|\hat{\rvx}_k-\rvu_k\|^2
\end{align}

Therefore, QKmeans iterates over the two steps, starting from some $\rmU^{(0)}$:
\begin{itemize}
	\item $\rvt^{(\tau)} = \argmin_\rvt g(\rmU^{(\tau-1)},\rvt)$
	\item $\rmU^{(\tau)}=\argmin_{\rmU\in\{\prod_j\rmS_j\}}g(\rmU,\rvt^{(\tau)})$
\end{itemize}
and $g(\rmU^{(\tau)},\rvt^{(\tau)})\leq g(\rmU^{(\tau-1)},\rvt^{(\tau)})\leq g(\rmU^{(\tau-1)},\rvt^{(\tau-1)})$
\todo[inline]{Rather say in the proposition that $g(U^{(\tau)},\rvt^{(\tau)})$ is nonincreasing?}
\end{proof}

\subsection{Convergence of \qkmeans}

\todo[inline]{The convergence proof is straightforward, and I suggest 
to put in (as done here) in the previous subsection. And I would skip the rest
of this section.}

\begin{algorithm}[t]
	\caption{\qkmeans algorithm and its time complexity.}
	\label{algo:qmeans}
	\begin{algorithmic}[1]
		
		
		\REQUIRE $\rmX \in \R^{\nexamples \times \datadim}$, $\nclusters$, initialization $\left \lbrace \rmS_q^{(0)} : \rmS_q^{(0)} \in \mathcal{E}_q\right \rbrace_{q\in\intint{\nfactors}}$
		\FOR{$\tau=1, 2, \ldots$ until convergence}
		\STATE $\rvt^{(\tau)} \leftarrow \argmin_{\rvt \in \intint{\nclusters}^\nexamples} \sum_{n\in\intint{\nexamples}} {\left \|\rvx_n - \rvu^{(\tau -1)}_{\rvt[i]}\right \|_2^2}$
		\COMMENT{$\mathcal{O}\left (\nexamples\left(A\log A+B\right ) + AB\right )$}
		\label{line:qmeans:assignment}
		\STATE $\forall k\in\intint{\nclusters}, \hat{\rvu}_k \leftarrow \frac{1}{n_k^{(\tau)}} \sum_{n: \rvt^{(\tau)}[n]= k} {\rvx_n}$
with $n_k \leftarrow |\{n: \rvt^{(\tau)}[n]=k\}|$
		\COMMENT{$\bigO{\nexamples\datadim}$}
%		\FORALL {$k \in [\![K]\!]$}
%		\label{line:qmeans:startkmeans}
%		\STATE $n_k^{(\tau)} \leftarrow |\{i: \rvt^{(\tau)}_i=k\}|$
%		\hfill \COMMENT{$\mathcal{O}\left (1\right )$}
%		\STATE $\hat{\rmX}^{(\tau)}_k \leftarrow \frac{1}{n^\tau_k} \sum_{i: \rvt^\tau_i = k} {\rvx_i}$
%		\hfill \COMMENT{$\mathcal{O}\left (1\right )$}
%		\ENDFOR
%		\label{line:qmeans:endkmeans}
		\STATE $\rmA \leftarrow \mathcal{D}_{\sqrt{\rvn^{(\tau)}}} \times \hat{\rmU}^{(\tau)} $
		\COMMENT{$\bigO{\nclusters\datadim}$}
		\STATE $\left \lbrace \rmS_q^{(\tau)}\right \rbrace_{q\in\intint{\nfactors}} \leftarrow \argmin_{\left \lbrace \rmS_q\right \rbrace_{q\in\intint{\nfactors}}} \left \|\rmA - \prod_{q\in\intint{\nfactors}}^{Q}{\rmS_q}\right \|_F^2 + \sum_{q\in\intint{\nfactors}} \mathbf{1}_{\mathcal{E}_q}(\rmS_q)$\\
		\COMMENT{$\bigO{AB\log^2A}$ (or $\bigO{AB\log^3A}$)}
		\STATE $\rmU^{(\tau)} \leftarrow \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}^{-1} \times \prod_{q\in\intint{\nfactors}}{\rmS_q^{(\tau)}}$
		\COMMENT{$\bigO{A^2\log A+AB}$}
		\ENDFOR
		\ENSURE assignement vector $\rvt$ and sparse matrices $\left \lbrace \rmS_q : \rmS_q \in \mathcal{E}_q\right \rbrace_{q\in\intint{\nfactors}}$ such that $\prod_{q\in\intint{\nfactors}}\rmS_q \approx \rmU$ the $\nclusters$ means of the $\nexamples$ data points
	\end{algorithmic}
\end{algorithm}

\addVE{TODO: fix how to write indicator function instead of $\mathbf{1}_{\mathcal{E}_q}(\rmS_q)$ in algo \qkmeans.}

\addVE{TODO: $\hat{\rmX}$ is renamed $\hat{\rmU}$ in algorithm \qkmeans, check consistency in the rest of the paper.}

\addVE{TODO: adapt \qkmeans to deal with the fixed factor $\mathcal{D}_{\sqrt{\rvn^{(\tau)}}}$}

\addVE{TODO: complexity of \qkmeans
\begin{itemize}
\item assignation: 
\begin{itemize}
\item compute the product of the sparse factors in $\mathcal{O}\left (A^2\log A+AB\right )$ and the norm of each cluster center $\mathcal{O}\left (\nclusters\datadim\right )=\mathcal{O}\left (AB\right )$
\item apply the operator to the data points in $\mathcal{O}\left (\nexamples\left(A\log A+B\right ) \right )$
\end{itemize} 
\end{itemize}
}
To show this convergence, we need to show that each update step in one iteration $\tau$ of the algorithm actually reduces the overall objective. To this end, we start by re-writing the objective at a given time-step $\tau$:
%
\begin{equation}
\begin{split}
\label{eq:qmean_problem_2}
    g(&\{ \mathcal{S}_1^{(\tau)}, \ldots,\mathcal{S}_Q^{(\tau)} \}, \lambda^{(\tau)}, \rvt^{(\tau)})\\
    = & \sum_{k=1}^{K} \left( \sum_{j: \rvt^{(\tau)}_j = k} ||\rmX_j - \rmU^{(\tau)}_k||^2 \right) + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j^{(\tau)})\\
    & s.t. ~ \rmU = \lambda^{(\tau)} \prod_{j=1}^{Q}{\mathcal{S}_j^{(\tau)}}
\end{split}
\end{equation}.
%
We then assess whether or not this objective diminishes at each time-step in Algorithm \ref{algo:qmeans}.




\paragraph{Assignment step (Line \ref{line:qmeans:assignment})} For a fixed $\rmU^{(\tau-1)}$ the new indicator vector $\rvt^{(\tau)}$ is defined as:
%
\begin{equation}
\label{eq:qmean_problem_U_fixed}
 \rvt^{(\tau)}_i = \argmin_{k \in [\![K]\!]} ||\rmX_i - \rmU^{(\tau-1)}||_2^2
\end{equation}
%
for any $i \in [\![n]\!]$. This step is exactly identical in the K-means algorithm (Algorithm \ref{algo:kmeans}) and is clearly minimizing the objective function \textit{w.r.t.} to vector $\rvt$.

\paragraph{Centroids computation step (Line \ref{line:qmeans:startkmeans} to \ref{line:qmeans:endkmeans})} For a fixed $\rvt^{(\tau)}$, the new sparsely-factorized centroids are solutions of the following subproblem:
%
\begin{equation}
\label{eq:qmeans_problem_t_fixed}
\begin{split}
 \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & g(\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda, \rvt^{(\tau)}) \\
 = \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} &\sum_{k=1}^{K} \left( \sum_{j: \rvt^{(\tau)}_j = k} ||\rmX_j - \rmU_k||^2_2 \right) + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j)  \\
 = \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & ||\mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~(\hat{\rmX}^{(\tau)} - \rmU)||_{\mathcal{F}} ^ 2  \\
 &+ \sum_{k=1}^{K} c_k^{(\tau)} + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j)\\
 & s.t. ~ \rmU = \lambda \prod_{j=1}^{Q}{\mathcal{S}_j}
\end{split} 
\end{equation}
%
where :
%
\begin{itemize}
 \item $\sqrt{\rvn^{(\tau)}} \in {\R^{K}}$ is the pair-wise square root of the vector indicating the number of observations in each cluster at step $\tau$: $\rvn_k^{(\tau)} = |\{i: \rvt^\tau_i = k\}|$;
 \item $\mathcal{D}_\rvv \in \R^{K \times K}$ refers to a diagonal matrix with entries in the diagonal from a vector $\rvv$;
 \item $\hat{\rmX}^{(\tau)} \in \R^{K \times d}$ refers to the real centroid matrix obtained at step $\tau$ \textit{w.r.t} the indicator vector at this step $\rvt^{(\tau)}$: $\hat{\rmX}^{(\tau)}_k = \frac{1}{\rvn_k}\sum_{j:\rvt^{(\tau)}_j = k} {\rmX_j}$. When $\rvt^{(\tau)}$ is fixed, this is constant.
 \item $c_k^{(\tau)} = \sum_{j: \rvt^{(\tau)}_j = k}^{}||\rmX_j - \hat{\rmX}_k^{(\tau)}||$ is constant \textit{w.r.t} $\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}$ and $\lambda$.
\end{itemize}

Again,  the minimization of the overall objective $g$ from Equation \ref{eq:qmean_problem_2} is clear since the $\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}$ and $\lambda$ are precisely chosen to minimize $g$.

Note that the formulation of the problem in Equation \ref{eq:qmeans_problem_t_fixed} shows the connection between the K-means and \textit{Hierarchical PALM4LED} objectives, which allows us to combine them without trouble. Indeed, we can set
%
\begin{equation*}
\rmA^{(\tau)} = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\hat{\rmX}^{(\tau)}
\end{equation*}
and
\begin{equation*}
\rmB^{(\tau)} = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\rmU = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\lambda \prod_{j=1}^{Q}{\mathcal{S}_j} = \lambda \prod_{j=0}^{Q}{\mathcal{S}_j}
\end{equation*}
%
with $\mathcal{S}_0$ fixed and equal to $\mathcal{D}_{\sqrt{\rvn^{(\tau)}}}$. The Equation \ref{eq:qmeans_problem_t_fixed} can then be rewritten as
%
\begin{equation}
\begin{split}
 \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & ||\rmA^{(\tau)} - \rmB^{(\tau)}||_{\mathcal{F}} ^ 2  +  \sum_{j=0}^{Q} \delta_j(\mathcal{S}_j)\\
 s.t. &~ \rmB^{(\tau)} = \lambda \prod_{j=0}^{Q}{\mathcal{S}_j}
\end{split}
\end{equation}

Since \textit{Hierarchical PALM4LED} successivly updates the $\mathcal{S}_j$s independently and in an alternating fashion, we can still use \textit{PALM4LED} in to solve this problem with the $\mathcal{S}_0$ fixed.





The factorization of $\rmU$ could then be used in an ulterior algorithm that involves a matrix-vector multiplication with $\rmU$: typically any algorithm involving the assignment of some data points to one of the clusters (Equation \ref{eq:assignment_problem_kmeans}). Such applications of our proposed algorithm are discussed in Section \ref{sec:uses}.

\subsection{Complexity analysis}

Since the space complexity of the proposed \qkmeans algorithm is comparable to that of K-means, we only detail its time complexity. We set $A=\min\left (\nclusters, \datadim\right )$ and $B=\max\left (\nclusters, \datadim\right )$, and assume that the number of factors satisfies $\nfactors=\bigO{\log A}$.

The analysis is proposed under the following assumptions: the product between two dense matrices of shapes ${N_1\times N_2}$ and ${N_2\times N_3}$ can be done $\mathcal{O}\left (N_1 N_2 N_3 \right )$ operations; 
the product between a sparse matrix with $\bigO{S}$ non-zero entries and a dense vector can be done in $\bigO{S}$ operations; 
the product between two sparse matrices of shapes ${N_1\times N_2}$ and ${N_2\times N_3}$, both having $\bigO{S}$ non-zero values can be done in $\bigO{S \min\left (N_1, N_3\right )}$ and the number of non-zero entries in the resulting matrix is $\bigO{S^2}$.

%We give a thorough analysis of the Q-means algorithm and we show the theoretical benefits of using our method compared to the classical K-means algorithm.

%We first give essential knowledge on sparse and dense matrix multiplication and we study the complexity of the PALM4MSA algorithm proposed in \cite{magoarou2014learning} \addLG{cette source ne correspond pas à la dernière version du papier (préférable)}. We then show how to take advantage of the sparse factorization of the K-means matrix both while forming it and using it in further algorithms. 

%\subsubsection{Preliminaries}

%We start by giving some general information about the complexity of some standard linear algebra operations then we analyse precisely the cost of the PALM4MSA algorithm proposed in 
%\cite{magoarou2014learning} \addLG{cette source ne correspond pas à la dernière version du papier (préférable)} and we finally recall the complexity involved in the K-means algorithm.

%\paragraph{Complexity of a matrix multiplication between a dense matrix and a dense vector.}
%Let $\rmA$ be a $K \times d $ matrix and $\rvv$ be a $d$ dimensional dense vector. The matrix-vector product $\rmA\rvv$ can be done in $\mathcal{O}\left(Kd \right)$ operations.
%
%\paragraph{Complexity of a matrix multiplication between two dense matrices.}
%Let $\rmA$ be a $K \times d $ matrix and $\rmB$ be a $d \times N$ matrix, then computing $\rmA \rmB$ can be done in $\mathcal{O}\left (KdN \right )$ operations.

%\paragraph{Complexity of a matrix multiplication between a sparse matrix and a dense vector.}
%Let $\rmA$ be a $K \times d$ sparse matrix with $\mathcal{O}(p)$ non-zero entries and $\rvv$ be a $d$ dimensional dense vector. The matrix-vector product $\rmA\rvv$ can be done in $\mathcal{O}\left(p \right)$ operations.
%
%\paragraph{Complexity of a matrix multiplication between a sparse matrix and a dense matrix.}
%Let $\rmA$ be a $K \times d$ sparse matrix with $\mathcal{O}(p)$ non-zero entries and $\rmB$ be a $d \times N$ dense matrix. The matrix-matrix product $\rmA\rmB$ can be done in $\mathcal{O}\left(p N\right)$ operations.

%\paragraph{Complexity of a matrix multiplication between a sparse matrix and a sparse matrix.}
%Let $\rmA$ be a $K \times d $ sparse matrix and $\rmB$ be a $d \times N$ sparse matrix, both having $\mathcal{O}(p)$ non-zero values.
%To the best of our knowledge, the best achievable complexity for the matrix-matrix product in this general scenario is $\mathcal{O}(p~\min{\{K, N\}})$. We remark here that the number of values in such resulting matrix is $\mathcal{O}(p^2)$.
%The $\min$ term appears because we can either compute $\rmA\rmB$ or $(\rmB^T\rmA^T)^T$ for the same result.

%\paragraph{Complexity of the evaluation of Q sparse factors: $\prod_{j=1}^{Q}\mathcal{S}_j$}
%Let $\mathcal{S}_j$ be a sparse matrice of $p$ non-zero values for any $j \in [\![Q]\!]$. Let also the resulting matrix be of size $K \times d$.  Finally, let $\mathcal{S}_1$ be a $K \times q$ matrix, $\mathcal{S}_Q$ be a $q \times d$. and all the other $\mathcal{S}_j$ be $q \times q$ matrices; $q$ is set to be the minimum of $\{K, d\}$. We consider, for the sake of simplicity, that $p$ is $\mathcal{O}(q)$: e.g. there is one value by row or column in the $\mathcal{S}_j$s. In this case (which is considered to be our case), the product $\prod_{j=1}^{Q}\mathcal{S}_j$ can be done in time $\mathcal{O}(Qpq)$: once a sparse-sparse matrix multiplication then $Q-2$ times the sparse-dense matrix multiplication.

%\paragraph{Complexity of the multiplication between Q sparse factors and a dense vector}
%Let $\rmS$ be a short-hand for $\prod_{j=1}^{Q}\mathcal{S}_j$ that has been detailed above. $\rmS$ is kept as a factorization. Let also $\rvv$ be a $d$ dimensional dense vector. Then the product $\rmS \rvv$ can be computed right to left in time $\mathcal{O}(Qp)$ operations.
%If $\rmB$ is also sparse, with $b$ non-zero entries, then the bound is $\mathcal{O}\left ( \min\left ( a \min\left (b, N \right ), b \min\left (a, M\right ) \right ) \right )$ where $M$ is the number of rows in $\rmA$.
%This is a naive upper bound for sparse matrices, some tighter bound may be found.

\paragraph{Complexity of the \kmeans algorithm.}
We recall here that the \kmeans algorithm complexity is dominated by its cluster assignation step which requires $\bigO{\nexamples\nclusters\datadim}=\bigO{\nexamples A B}$ operations (see Eq.~\eqref{eq:assignment_problem_kmeans}).

\paragraph{Complexity of algorithm \palm.} The procedure consists in an alternate optimization of each sparse factor. 
At each iteration, the whole set of $\nfactors$ factors is updated with at a cost in $\bigO{AB\log^2 A}$, as detailed in Appendix~\ref{sec:app:palm4msa}. 
The bottleneck is the computation of the gradient, which benefits from fast computations with sparse matrices.
The hierarchical version of \palm proposed in~\cite{LeMagoarou2016Flexible} consists in running $\palm$ $2Q$ times so that its time complexity is in $\bigO{AB\log^3 A}$.

%Each iteration takes $\mathcal{O}(Q(Qpq + Kd + q^2\log q^2) + K^2d)$\addLG{$q^2 \log q^2$ peut être remplacé par $q^2 + p\log p$ grâce à l'algo quickselect}. In the following analysis, we refer to the lines in Algorithm 2 of \cite{magoarou2014learning}. This algorithm is repeated here for simplicity (Algorithm \ref{algo:palm4msa}). Note that the displayed complexities are for one full iteration of the algorithm.
%
%\begin{description}[leftmargin=\parindent,labelindent=\parindent]
% 
% \item [Line 3] The $\rmL$s can be \textit{precomputed} incrementaly for each iteration $i$, involving a total cost of $\mathcal{O}(Qpq)$ operations: for all $j < Q$, $\rmL_j = \rmL_{j+1} \mathcal{S}^i_{j+1}$; for $j = Q$, $\rmL_j = \textbf{Id}$;
% \item [Line 4] The $\rmR$s is computed incrementaly for each iteration $j$: $\rmR_j = \mathcal{S}^{i+1}_{j-1} \rmR_{j-1}$ if $j > 1$; $\rmR_j = \textbf{Id}$ otherwise. This costs an overall $\mathcal{O}(Qpq)$ operations;
% \item [Line 5] The time complexity for computing the operator norm of a matrix of dimension $K \times q$ is $\mathcal{O}(Kq)$, which leads a $\mathcal{O}(QKq)$ number of operations for this line \addLG{à éclaircir...};
% \item [Line 6] \addLG{avec Valentin on avait trouvé O(Kd min \{K, d\}) mais je ne suis plus d'accord} Taking advantage of the decompositions of $\rmL$ and $\rmR$ as products of sparse factors, the time complexity of this line ends up being $\mathcal{O}(Q(Qpq + Kd + q^2\log q^2))$ for a complete iteration: the $\mathcal{O}(Qpq)$ part comes from the various sparse-dense matrix multiplications with $\rmR$ and $\rmL$; the $\mathcal{O}(Kd)$ part comes from the pairwise substraction inside the parentheses and the $\mathcal{O}(q^2 \log q^2)$ part from the projection operator that involves sorting of the inner matrix.\addLG{$q^2 \log q^2$ peut être remplacé par $q^2 + p\log p$ grâce à l'algo quickselect}
% \item [Line 8] The reconstructed $\hat \rmU$ can be computed from the $\rmR_{Q-1}$ and $\mathcal{S}_Q^{i+1}$ obtained just before: $\hat \rmU = \mathcal{S}_Q^{i+1} \rmR_{Q-1}$. This sparse-dense matrix multiplication cost a time $\mathcal{O}(pq)$.
% \item [Line 9] \addLG{Avec valentin, on avait écrit $O(min\{K, d\} ^ 3)$ mais je ne suis plus d'accord}The computational complexity of this line is majored by the matrix multiplications that cost $\mathcal{O}(K^2d)$ operations.
%\end{description}
%
%Adding up the complexity for each of those lines and then simplifying gives an overall complexity of $\mathcal{O}(Q(Qpq + Kd + q^2\log q^2) + K^2d)$\addLG{$q^2 \log q^2$ peut être remplacé par $q^2 + p\log p$ grâce à l'algo quickselect}. Note that $\mathcal{O}(Kq)$ is majored by $\mathcal{O}(Kd)$ since $d \geq q$.

%\begin{algorithm}
%\caption{PALM4MSA algorithm}
%\label{algo:palm4msa}
%\begin{algorithmic}[1]
%
%
%\REQUIRE The matrix to factorize $\rmU \in \R^{K \times d}$, the desired number of factors $Q$, the constraint sets $\mathcal{E}_j$ , $j \in [\![Q]\!]$ and a stopping criterion (e.g., here, a number of iterations $N_{iter}$ ).
%
%\ENSURE $\{\mathcal{S}_1 \dots \mathcal{S}_{Q}\}|\mathcal{S}_j \in \mathcal{E}_j$ such that $\prod_{j=1}^{Q}\mathcal{S}_j \approx \rmU$
%
%\FOR {$i = 0$ to $N_{iter}$}
%\FOR {$j = 1$ to $Q$}
%\STATE  $\rmL_j \leftarrow \prod_{l=j+1}^{Q} \mathcal{S}_{l}^{i}$
%\STATE  $\rmR_j \leftarrow \prod_{l=1}^{j-1} \mathcal{S}_{l}^{i+1}$
%\STATE $c_j^i :> (\lambda^i)^2 ||\rmR_j||_2^2 ||\rmL_j||_2^2$
%\STATE $\mathcal{S}^{i+1}_j \leftarrow P_{\mathcal{E}_j}(\mathcal{S}_j^i - \frac{1}{c_j^i} \lambda^i \rmL_j^T(\lambda \rmL_j \mathcal{S}_j^i \rmR_j - \rmU)\rmR_j^T)$
%\ENDFOR
%\STATE $\hat \rmU := \prod_{j=1}^{Q} \mathcal{S}_j^{i+1}$
%\STATE $\lambda^{i+1} \leftarrow \frac{Trace(\rmU^T\hat\rmU)}{Trace(\hat\rmU^T\hat\rmU)}$
%\ENDFOR
%
%\ENSURE $\lambda, \{\mathcal{S}_1 \dots \mathcal{S}_{Q}\}|\mathcal{S}_j \in \mathcal{E}_j$ such that $\lambda \prod_{j=1}^{Q}\mathcal{S}_j \approx \rmU$
%
%\end{algorithmic}
%\end{algorithm}
%

%Also, we consider the scenario when the first factor is in $\R^{K \times d}$ and all the others are in $\R^{d \times d}$. Finaly, we set the number of values in each sparse matrix to be the same and equal to $p$.

%For each of the $Q$ factors, the complexity is $\mathcal{O}(dpQ + Kd + d^2))$:
%
%\begin{itemize}
% \item Lines 3 and 4: $\mathcal{O}(dpQ)$ by computing the products right to left and taking advantage of the sparsity of the factors;
% \item Line 5: $\mathcal{O}(Kd + d^2)$ because $\rmR \in \R^{K \times d}$ and $\rmL \in \R^{d \times d}$;
% \item Line 6: $\mathcal{O}(dpQ + Kd)$ for the sparse product and the projection operation.
%\end{itemize}
%
%The two last statement of each iteration are in time $\mathcal{O}(dpQ)$, again taking advantage of the sparse factorisation.
%
%\begin{itemize}
% \item Line 8: $\mathcal{O}(dpQ)$;
% \item Line 9: $\mathcal{O}(dpQ)$.
%\end{itemize}

%\paragraph{Complexity of algorithm \textit{Hierarchical PALM4MSA}.}
%
%The hierarchical version of the algorithm corresponds to the same algorithm repeated $Q$ times. The overall complexity is then $\mathcal{O}(Q^2(Qpq + Kd + q^2\log q^2) + K^2d)$. 

\paragraph{Complexity of the \qkmeans algorithm.} The overall complexity of \qkmeans is in $\bigO{\nexamples\left(A\log A+B\right ) + AB \log^2 A}$ when used with \palm and in $\bigO{\nexamples\left(A\log A+B\right ) + AB \log^3 A}$ when used with the hierarchical version of \palm. The time complexities of the main steps are given in Algorithm~\ref{algo:qmeans}. 

The assignation step (line~\ref{line:qmeans:assignment} and Eq.~\eqref{eq:assignment_problem_kmeans}) benefits from the fast computation of $\rmU \rmX$ in~$\bigO{\nexamples\left(A\log A+B\right )}$ while the computation of the norms of the cluster centers is in $\bigO{AB}$.
This bottleneck in \kmeans is here TO BE CONTINUATED

We now show how the cluster assignation step of our method is computationaly less expensive than the one of the previous K-means algorithm and how this feature might even fasten the computation of the K-means algorithm in general.

\paragraph{Cluster assignation}

In Equation~\ref{eq:assignment_problem_kmeans}, we have seen that the cost of the assignation of an observation to a cluster is majored by the matrix-vector multiplication between the cluster matrix and the observation. Using our method, this cost is reduced from $\mathcal{O}(Kd)$ operations to $\mathcal{O}(Qp)$ operations. \addLG{Envolée: In the experiments, we will se that $Q$ can be chosen sufficiently little so that this complexity becomes to $\mathcal{O}(p \log q)$ operations.}

\paragraph{Q-means factorization construction}

This fastening of the assignation step can also be used while constructing the Q-means factorization: the complexity of this step is reduced from $\mathcal{O}(ndk)$ to $\mathcal{O}(npQ)$. Nevertheless, this reduction has to be taken cautiously because of the extra-step in the Q-means algorithm: the inner call to the hierarchical-PALM4MSA at each iteration that costs $\mathcal{O}(Q^2(Qpq + Kd + q^2\log q^2) + K^2d)$. This leaves us with an overall time complexity of $\mathcal{O}(Q^2(Qpq + Kd + q^2\log q^2) + K^2d) + npQ$. \addLG{We note here that our use of the PALM4MSA algorithm doesn't rely on the number of sample in the full dataset: if $n$ is large enough compared to $K$ and $d$, then the complexity is majored by $\mathcal{O}(np \log q)$}

%We now note that, in practice, $Q$ is supposed to be small compared to $K$ or $d$, and that $p$ should be of the same order than $d$. In that case, we can simplify the complexity of the final algorithm to $\mathcal{O}(Kd + d^2)$.

%\paragraph{Complexity of Kmeans (algorithm~\ref{algo:kmeans}).}
%Each iteration takes $\mathcal{O}\left (ndk\right )$
%\begin{itemize}
%\item Assignment, line~\ref{line:kmeans:assignment}: $\mathcal{O}\left (ndk\right )$\\
%it is dominated by the computation of $\rmU \rmX^T$ using $\left \|\rvx-\rvu\right \|_2^2=\left \|\rvx\right \|_2^2+\left \|\rvu\right \|_2^2-2\left <\rvx, \rvu\right >$
%\item Computing size of cluster, line~\ref{line:kmeans:count}: $\mathcal{O}\left (n \right )$\\ it consists of one pass over $\rvt$.
% \item Updating the centroids, line~\ref{line:kmeans:compute_means}: $\mathcal{O}\left (nd\right )$\\
%since each example is summed once.
%\end{itemize}

%\paragraph{Complexity of Q-means (algorithm~\ref{algo:qmeans}).}

%Naively, each iteration of the \textit{Q-means} algorithm has the same complexity than K-means with an additional $\mathcal{O}(Kd + d^2)$ for the \textit{Hierarchical PALM4LED} step. This leads to the overall complexity of $\mathcal{O}(Kdn + d^2)$. In the case where the dimensionality of the data is negligible in front of the size of the dataset, this is of the same order of complexity than the vanilla K-means algorithm, e.g. $\mathcal{O}(ndk)$. Nevertheless, we can already take advantage of the decomposition of the center-point matrix in the assignment step of the Algorithm: indeed, when the intermediate center-point matrix $\rmU^{(\tau-1)}$ has the sparse factorization constraint, assigning a $n$ data points to their cluster can be done in time $\mathcal{O}(n K \log d)$ operations, which leads to the new complexity for \textit{Q-means}: $\mathcal{O}(nK\log d + Kd + d^2)$

%Once the decomposed center-point matrix has been obtained, the assignation of a new data point to a cluster becomes $\mathcal{O}(nK\log d)$ which would be usefull in many machine learning application usually using the K-means algorithm.
