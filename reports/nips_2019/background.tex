\section{Preliminaries}
\label{sec:background}

In this section, we begin by introducing some notation and then review the basics of K-means and give some background about learning fast transforms.

\subsection{Notations}


\input{notations.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
	\centering
	\begin{tabular}{|r|c|l|}
		\hline
		indices &  $i$, $j$, $m$, $n$, $p$, $q$ &  small  Latin characters  \\
		other integers &  $K$, $Q$, $N$, $\ldots$ &  capital  Latin characters \\
	%	vector spaces\footnotemark & $\mathcal{X}$, $\mathcal{Y}$, $\mathcal{H}$, $\ldots$ & Calligraphic letters \\ 
		vectors (or functions) & $\rvx$, $\rvt$, $\rvk$, $\ldots$ & small bold Latin characters \\
		matrices  & $\rmX$, $\rmU$, $\rmK$, $\ldots$ & capital bold Latin characters \\
		transpose & $\top$ & $\rmX^\top$ transpose of  $\rmX$ \\
		\hline
	\end{tabular}
	\caption{Notations used in this paper.}
	\label{tab:notation}
\end{table}
\addtocounter{footnote}{0}
\footnotetext{We also use the standard notations such as $\mathbb{R}^n$ and $\mathbb{M}_n$.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\subsection{K-means}
\label{sec:kmeans}
The K-means algorithm is used to partition a given set of observations $\rmX$ into a predefined number $\nclusters$ of clusters while minimizing the distance between the observations in each partition:

\begin{equation}
\label{eq:kmean_problem}
    \argmin_{\rmU, \rvt} \sum_{k\in\intint{\nclusters}} \sum_{j: \rvt[n] = k} ||\rmX_{[n]} -\rmU_{[k]}||^2,
\end{equation}
% autre Ã©criture de l'objectif de k-means
% = \argmin_{\rmU, \rvt} \sum_{k=1}^{K} c_k + \sum_{k=1}^{K} n_k||\hat{\rmU}_k - \rmU_k||^2
where the matrix $\rmU \in \R^{\nclusters \times \datadim}$ is composed of the cluster centers and the indicator vector $\rvt \in  [\![\nclusters]\!]^\nexamples$ assigns each example to a cluster.

The algorithm starts with an initialized set of $\nclusters$ cluster centers $\rmU_{[k]}^{(0)} \in \R^\datadim$, $k\in\intint{\nclusters}$. Along iterations indexed by $\tau$, it then alternates between (i) an assignment step where each point assignment is updated as
\begin{align}
\label{eq:assignment_problem_kmeans}
\forall n\in\intint{N}, \rvt[n] \leftarrow \argmin_{k \in [\![\nclusters]\!]} ||\rmX_{[n]} - \rmU_{[k]}^{(\tau-1)}||_2^2 = \argmin_{k \in [\![\nclusters]\!]} ||\rmU_{[k]}^{(\tau-1)}||_2^2 - 2 \left <\rmU_{[k]}^{(\tau-1)}, \rmX_{[n]}\right >
\end{align}
and (2) the update of each cluster center as the centroid of the data points assigned to this cluster:
\begin{align}
\label{eq:center_update}
\forall k\in\intint{K}, \rmU^{(\tau)}_k \leftarrow \frac{1}{n_k^{(\tau)}} \sum_{n: \rvt^{(\tau)}[n]= k} {\rmX_{[n]}}
\end{align}
where $n_k^{(\tau)} \leftarrow |\{n: \rvt^{(\tau)}[n]=k\}|$ is the number of points in cluster $k$, at a total cost of $\mathcal{O}\left (\nexamples\datadim\right )$ operations.

The assignment step (\eqref{eq:assignment_problem_kmeans}) costs $\mathcal{O}(\nexamples\datadim\nclusters)$ operations while the center update (\eqref{eq:center_update}) is in $\mathcal{O}\left (\nexamples\datadim\right )$ operations. Hence, the bottleneck in the overall time complexity $\mathcal{O}(\nexamples\datadim\nclusters)$ is due to the assignment step. Once the clusters have been defined, assigning $\nexamples'$ new points to these clusters is performed via \eqref{eq:assignment_problem_kmeans} at the cost of $\mathcal{O}\left (\nexamples'\datadim\nclusters \right )$ operations.

The main contribution in this paper relies on the idea that \eqref{eq:assignment_problem_kmeans} may be computed more efficiently by approximating $\rmU$ as a fast operator.

% Each update step $\tau$ is divided into two parts: (i) all observations $\rmX_{[n]}$ are assigned to their nearest cluster based on the center-points $\rmU_{[\rvt[n]]}^{(\tau-1)}$s at this step (Line \ref{line:kmeans:assignment}) in $\mathcal{O}(\nexamples\datadim\nclusters)$ operations
%
% (ii) the new center-points $\rmU_{[k]}^{(\tau)}$s are computed as the means of the assignated $\rmX_{[n]}$ (Line \ref{line:kmeans:compute_means}) for a total of $\mathcal{O}\left (\nexamples\datadim\right )$ operations.

%\begin{algorithm}
%\caption{K-means algorithm}
%\label{algo:kmeans}
%\begin{algorithmic}[1]
%
%
%\REQUIRE $\rmX \in \R^{\nexamples \times \datadim}$, $\nclusters$, $\{\rmU_i \in \R^\datadim\}_{i=1}^{\nclusters}$
%\ENSURE $\{\rmU_i\}_{i=1}^{\nclusters}$ the K means of $\nexamples$ $\datadim$-dimensional samples
%\STATE $\tau \leftarrow 0$
%\REPEAT
%\STATE $\tau \leftarrow \tau + 1$
%\STATE $\rvt^{(\tau)} \leftarrow \argmin_{\rvt \in [\![\nclusters]\!]^n} \sum_{i=1}^{\nexamples} {||\rmX_i - \rmU^{(\tau-1)}_{\rvt_i}||_2^2}$
%\label{line:kmeans:assignment}
%\FORALL {$k \in [\![\nclusters]\!]^\nexamples$}
%\STATE $n_k^{(\tau)} \leftarrow |\{i: \rvt^{(\tau)}_i=k\}|$
%\label{line:kmeans:count}
%\STATE $\rmU^{(\tau)}_k \leftarrow \frac{1}{n_k^{(\tau)}} \sum_{i: \rvt^{(\tau)}_i = k} {\rmX_i}$
%\label{line:kmeans:compute_means}
%\ENDFOR
%\UNTIL{stop criterion}
%\RETURN $\rmU^{(\tau)}$
%\end{algorithmic}
%\end{algorithm}


%Once the clusters have been defined, for any $\rvx \in \R^\datadim$ the cluster associated with this $\rvx$ is:
%
%\begin{equation}
%\label{eq:assignment_problem_kmeans}
%\argmin_{k \in [\![\nclusters]\!]} ||\rvx - \rmU_{[k]}||_2^2 = \argmin_{k \in [\![\nclusters]\!]} ||\rmU_{[k]}||_2^2 - 2 \rmU_{[k]}^T\rvx.
%\end{equation}
%
%
%We remark here that the computational bottleneck of this assignment lies in the computation of $\rmU_{[k]}^T\rvx$ for all $k$. This computation is also encountered in the assignment step (line \ref{line:kmeans:assignment}) of the Algorithm \ref{algo:kmeans}.


\subsection{Learning fast transforms as the product of sparse matrices}
\label{sec:palm4msa}

The success of some linear operators like Fourier or Hadamard transforms relies on both their mathematical properties and their ability to be computed in an efficient way, typically in $\mathcal{O}\left (M\log\left (M\right )\right )$ instead of $\mathcal{O}\left (M^2\right )$ operations when defined in dimension $M$. 
The main idea of the related fast algorithm is that the matrix $\rmU\in\sR^{M\times M}$ characterizing such a linear operator can be written as the product $\rmU=\Pi_{q\in\intint{\nfactors}}\rmS_q$ of $\nfactors$ sparse matrices $\rmS_q$, with $Q=\mathcal{O}\left (\log M\right )$ and $\left \|\rmS_q\right \|_0=\mathcal{O}\left (M\right )$.
As a consequence, for any vector $\rvx\in\sR^M$, $\rmU\rvx$ can be computed efficiently as $\mathcal{O}\left (\log M\right )$ products $\rmS_0 \left (\rmS_1 \left (\ldots \left (\rmS_{Q-1}\rvx\right )\right )\right )$ between a sparse matrix and a vector, the cost of each product being $\mathcal{O}\left (M\right )$.

When the linear operator $\rmU$ is an arbitrary matrix, one may approximate it with such a sparse-product structure by learning the factors $\left \lbrace\rmS_q\right \rbrace_{q\in\intint{Q}}$ in order to benefit from a fast algorithm.
Recent works~\cite{LeMagoarou2016Flexible} have proposed algorithmic strategies to learn such a factorization.

TO BE COMPLETED + algo~\ref{algo:palm4msa} in appendix~\ref{sec:app:palm4msa}.

\cite{bolte2014proximal, LeMagoarou2016Flexible}

A popular way for providing concise description of high-dimensional vectors $\rmU \in \R^{K \times d}$ is to compute a sparse representation using a dictionary:
%
\begin{equation}
\rmU^T \approx \rmD\rmGamma
\end{equation}
%
where $\rmD \in \R^{d \times d}$ is a dictionary and $\rmGamma \in \R^{d \times K}$ has sparse columns. Historically, the dictionary is either (i) analytic: $\rmD$ is chosen to give a fast reconstruction of the initial matrix by taking advantage of some fast-transform algorithm (the \textit{Fast Hadamard Transform} for instance) or (ii) learned: $\rmD$ is learned from the data itself to give a good reconstruction of the initial matrix.

Building on the observation that the fast-transform associated with an analytic dictionary can be expressed as the product of sparse matrices $\mathcal{S}_j$ from a set $\mathcal{S}$ of size $M$, \cite{magoarou2014learning} proposes an algorithm to learn a dictionary from the data with sparsity constraints such that this dictionary would be both well-suited with the data and fast to use:
%
\begin{equation}
\rmD = \lambda \prod_{j=1}^{M}\mathcal{S}_j
\end{equation}
%
with $\forall j \in \{1 \ldots M\}$, $\mathcal{S}_j \in \mathcal{E}_j$, $\mathcal{E}_j = \lbrace \rmA \in \R^{a \times a+1}~\text{s.t.}~||\rmA||_0^0 \leq p_j, ||\rmA||_1 = 1 \rbrace$ and $p_j$ being chosen suitably. The $\lambda$ parameter has been added along with the normalization constraint in the $\mathcal{E}_j$ in order to remove scaling ambiguity in the learned $\mathcal{S}_j$.

Considering $\rmGamma$ being a sparse matrice too, it can be renamed as $\rmGamma = \mathcal{S}_{M+1}$. We set $Q = M+1$ and the overall dictionary learning problem can be expressed as the following optimisation problem:
%
\begin{equation}
\label{eq:problem_gribon}
\min_{\{\mathcal{S}_1 \dots \mathcal{S}_Q, \lambda\}} ||\rmU - \lambda \prod_{j=1}^{Q}{\mathcal{S}_j}||_2^2 + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j)
\end{equation}
%
with the $\delta_j(\mathcal{S}_j) = 0$ if $\mathcal{S}_j \in \mathcal{E}_j$ being the sparsity constraints to satisfy on the associated $\mathcal{S}_j$.

Although this problem is highly non-convex, the authors derive an algorithm from the PALM algorithm \cite{bolte2014proximal}, which they call \textit{Hierarchical PALM4LED} to find a good local minima and give convergence guarantees to learn efficient dictionaries.



