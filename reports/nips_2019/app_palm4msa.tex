\section{\palm algorithm}
\label{sec:app:palm4msa}

The \palm algorithm~\citep{LeMagoarou2016Flexible} is given in Algorithm~\ref{algo:palm4msa} together with the time complexity of each line, using $A=\min(\nclusters, \datadim)$ and $B=\max(\nclusters, \datadim)$. The complexity analysis is proposed under the following assumptions, which are satisfied in the mentioned applications and experiments: the number of factors is $\nfactors=\mathcal{O}\left (\log A\right )$; all but one sparse factors are of shape $A \times A$ and have $\bigO{A}$ non-zero entries while one of them is of shape $A\times B$ or $B\times A$ with $\bigO{B}$ non-zero entries.

\addVE{Pour que $S_0=\lambda \rmI$ soit à gauche, inverser l'ordre du produit ($S_1, S_2, \ldots, S_{Q-1}$ contre $S_{Q-1}...S_2 S_1$) précédemment: done. The current order is from left to right for indices ($S_1, S_2, \ldots, S_{Q-1}$) while the update is from right to left ($q = \nfactors$ down to $1$). Is it ok?}

\begin{algorithm}
	\caption{\palm algorithm}
	\label{algo:palm4msa}
	\begin{algorithmic}[1]
		
		\REQUIRE The matrix to factorize $\rmU \in \R^{\nclusters \times \datadim}$, the desired number of factors $\nfactors$, the constraint sets $\mathcal{E}_q$ , $q\in \intint{\nfactors}$ and a stopping criterion (e.g., here, a number of iterations $I$ ).
		
%		\ENSURE $\{\rmS_1 \dots \rmS_{\nfactors}\}|\rmS_q \in \mathcal{E}_q$ such that $\prod_{q\in\intint{\nfactors}}\rmS_q \approx \rmU$
		\STATE \addVE{$\lambda \leftarrow \norm{S_1}_F$} 
		\COMMENT{$\bigO{B}$}
		\label{line:palm:init:lambda}
		\STATE \addVE{$S_1 \leftarrow \frac{1}{\lambda} S_1$} 
		\COMMENT{$\bigO{B}$}
		\label{line:palm:normalize:S1}
		\FOR {$i \in\intint{I}$}
		\FOR {\addVE{$q = \nfactors$ down to $1$}}
%		\FOR {$q = 2$ to $\nfactors$}
%		\STATE  $\rmL_q \leftarrow \prod_{l=q+1}^{\nfactors} \rmS_{l}^{(i)}$
		\STATE  $\rmL_q \leftarrow \prod_{l=1}^{q-1} \rmS_{l}^{(i)}$
		\COMMENT{$\bigO{1}$}
		\label{line:palm:L}
%		\STATE  $\rmR_q \leftarrow \prod_{l=0}^{q-1} \rmS_{l}^{(i+1)}$
		\STATE  $\rmR_q \leftarrow \prod_{l=q+1}^{\nfactors} \rmS_{l}^{(i+1)}$
		\COMMENT{$\bigO{1}$}
		\label{line:palm:R}
%		\STATE Choose $c > (\lambda^{(i)})^2 ||\rmR_q||_2^2 ||\rmL_q||_2^2$
		\STATE Choose $c > \addVE{\lambda^2} ||\rmR_q||_2^2 ||\rmL_q||_2^2$
%		\COMMENT{in $\mathcal{O}\left (A\nfactors+B\right )$}
		\COMMENT{$\bigO{A \log A+B}$}
		\label{line:palm:c}
		\STATE $\rmD \leftarrow \rmS_q^i - \frac{1}{c} \addVE{\lambda} \rmL_q^T\left (\addVE{\lambda}\rmL_q \rmS_q^i \rmR_q - \rmU\right )\rmR_q^T$
%		\COMMENT{in $\mathcal{O}\left (\nclusters\datadim\nfactors\right )$}
		\COMMENT{$\bigO{AB\log A}$}
		\label{line:palm:D}
%		\STATE $\rmD \leftarrow \rmS_q^i - \frac{1}{c} \lambda^{(i)} \rmL_q^T(\lambda^{(i)} \rmL_q \rmS_q^i \rmR_q - \rmU)\rmR_q^T$
		\STATE $\rmS^{(i+1)}_q \leftarrow P_{\mathcal{E}_q}(\rmD)$
%		\COMMENT{in $\mathcal{O}\left (\nclusters\datadim\nfactors\right )$}
		\COMMENT{$\bigO{A^2\log A}$}
		\label{line:palm:update:S}
		\ENDFOR
		\STATE $\hat \rmU \eqdef \prod_{j=1}^{\nfactors} \rmS_q^{(i+1)}$
		\COMMENT{$\bigO{A^2\log A + AB}$}
		\label{line:palm:U}
		\STATE $\addVE{\lambda} \leftarrow \frac{Trace(\rmU^T\hat\rmU)}{Trace(\hat\rmU^T\hat\rmU)} \rmI$
%		\STATE $\rmS_1 \leftarrow \frac{Trace(\rmU^T\hat\rmU)}{Trace(\hat\rmU^T\hat\rmU)} \rmI$
%		\COMMENT{in $\mathcal{O}\left (\nclusters\datadim\right )$}
		\COMMENT{$\bigO{AB}$}
		\label{line:palm:update:lambda}
		\ENDFOR
		\STATE \addVE{$S_1 \leftarrow \lambda S_1$}
		\COMMENT{$\bigO{B}$}
		\label{line:palm:postprocess:S1}
		\ENSURE $\left \lbrace \rmS_q : \rmS_q \in \mathcal{E}_q\right \rbrace_{q\in\intint{\nfactors}}$ such that $\prod_{q\in\intint{\nfactors}}\rmS_q \approx \rmU$
		
	\end{algorithmic}
\end{algorithm}

In such conditions, the complexity of each line is:
\begin{itemize}
 \item [Lines~\ref{line:palm:init:lambda}-\ref{line:palm:normalize:S1}] Computing these normalization steps is linear in the number of non-zeros coefficients in $\rmS_1$.
 \item [Lines~\ref{line:palm:L}-\ref{line:palm:R}] Fast operators $\rmL$ and $\rmR$ are defined for subsequent use without computing explicitly the product.
% s can be \textit{precomputed} incrementaly for each iteration $i$, involving a total cost of $\mathcal{O}(Qpq)$ operations: for all $j < Q$, $\rmL_j = \rmL_{j+1} \mathcal{S}^i_{j+1}$; for $j = Q$, $\rmL_j = \textbf{Id}$;
% \item [Line 4] The $\rmR$s is computed incrementaly for each iteration $j$: $\rmR_j = \mathcal{S}^{i+1}_{j-1} \rmR_{j-1}$ if $j > 1$; $\rmR_j = \textbf{Id}$ otherwise. This costs an overall $\mathcal{O}(Qpq)$ operations;
 \item [Line~\ref{line:palm:c}] The spectral norm of $\rmL$ and $\rmR$ is obtained via a power method by iteratively applying each operator, benefiting from the fast transform.
 \item [Line~\ref{line:palm:D}] The cost of the gradient step is dominated by the product of sparse matrices.
% \addLG{avec Valentin on avait trouvé O(Kd min \{K, d\}) mais je ne suis plus d'accord} Taking advantage of the decompositions of $\rmL$ and $\rmR$ as products of sparse factors, the time complexity of this line ends up being $\mathcal{O}(Q(Qpq + Kd + q^2\log q^2))$ for a complete iteration: the $\mathcal{O}(Qpq)$ part comes from the various sparse-dense matrix multiplications with $\rmR$ and $\rmL$; the $\mathcal{O}(Kd)$ part comes from the pairwise substraction inside the parentheses and the $\mathcal{O}(q^2 \log q^2)$ part from the projection operator that involves sorting of the inner matrix.\addLG{$q^2 \log q^2$ peut être remplacé par $q^2 + p\log p$ grâce à l'algo quickselect}
\item [Line~\ref{line:palm:update:S}] The projection onto a sparse-constraint set takes $\bigO{A^2\log A}$ for all the $A\times A$ matrices.
 \item [Line~\ref{line:palm:U}] The reconstructed matrix $\hat \rmU$ is computed using $\bigO{\log A}$ products between $A\times A$ sparse matrices, in $\bigO{A^2}$ operations each, and one product with a sparse matrix in $\bigO{AB}$.
 \item [Line~\ref{line:palm:update:lambda}] The numerator and denominator can be computed using a Hadamard product between the matrices followed by a sum over all the entries.
  \item [Line~\ref{line:palm:postprocess:S1}] Computing renormalization step is linear in the number of non-zeros coefficients in $\rmS_1$.
\end{itemize}
\addVE{Revoir le coût de la ligne~\ref{line:palm:update:S} (tri/select+il manque le facteur de taille $AB$)}
\todo[inline]{Valentin: I have reintroduced $\lambda$, but just as an internal variable. Is it ok? Check also the compliance of this formulation with the rest of the paper, e.g., with the definition of $\mathcal{E}_q$.}
\todo[inline]{Clearly indicate that the factors are updated from right to left.}
\todo[inline]{Define $\mathcal{E}_q$ with the sparsity constraint and the Frobenius-norm constraint.}
\todo[inline]{add details about the hierarchical algorithm?}
\todo[inline]{The stopping criterion is the number of iterations $I$ here, which is incoherent with section~\ref{sec:uses:settings}.}
%Each iteration takes $\mathcal{O}(Q(Qpq + Kd + q^2\log q^2) + K^2d)$\addLG{$q^2 \log q^2$ peut être remplacé par $q^2 + p\log p$ grâce à l'algo quickselect}. In the following analysis, we refer to the lines in Algorithm 2 of \cite{magoarou2014learning}. This algorithm is repeated here for simplicity (Algorithm \ref{algo:palm4msa}). Note that the displayed complexities are for one full iteration of the algorithm.
%
%\begin{description}[leftmargin=\parindent,labelindent=\parindent]
% 
% \item [Line 3] The $\rmL$s can be \textit{precomputed} incrementaly for each iteration $i$, involving a total cost of $\mathcal{O}(Qpq)$ operations: for all $j < Q$, $\rmL_j = \rmL_{j+1} \mathcal{S}^i_{j+1}$; for $j = Q$, $\rmL_j = \textbf{Id}$;
% \item [Line 4] The $\rmR$s is computed incrementaly for each iteration $j$: $\rmR_j = \mathcal{S}^{i+1}_{j-1} \rmR_{j-1}$ if $j > 1$; $\rmR_j = \textbf{Id}$ otherwise. This costs an overall $\mathcal{O}(Qpq)$ operations;
% \item [Line 5] The time complexity for computing the operator norm of a matrix of dimension $K \times q$ is $\mathcal{O}(Kq)$, which leads a $\mathcal{O}(QKq)$ number of operations for this line \addLG{à éclaircir...};
% \item [Line 6] \addLG{avec Valentin on avait trouvé O(Kd min \{K, d\}) mais je ne suis plus d'accord} Taking advantage of the decompositions of $\rmL$ and $\rmR$ as products of sparse factors, the time complexity of this line ends up being $\mathcal{O}(Q(Qpq + Kd + q^2\log q^2))$ for a complete iteration: the $\mathcal{O}(Qpq)$ part comes from the various sparse-dense matrix multiplications with $\rmR$ and $\rmL$; the $\mathcal{O}(Kd)$ part comes from the pairwise substraction inside the parentheses and the $\mathcal{O}(q^2 \log q^2)$ part from the projection operator that involves sorting of the inner matrix.\addLG{$q^2 \log q^2$ peut être remplacé par $q^2 + p\log p$ grâce à l'algo quickselect}
% \item [Line 8] The reconstructed $\hat \rmU$ can be computed from the $\rmR_{Q-1}$ and $\mathcal{S}_Q^{i+1}$ obtained just before: $\hat \rmU = \mathcal{S}_Q^{i+1} \rmR_{Q-1}$. This sparse-dense matrix multiplication cost a time $\mathcal{O}(pq)$.
% \item [Line 9] \addLG{Avec valentin, on avait écrit $O(min\{K, d\} ^ 3)$ mais je ne suis plus d'accord}The computational complexity of this line is majored by the matrix multiplications that cost $\mathcal{O}(K^2d)$ operations.
%\end{description}
%
%Adding up the complexity for each of those lines and then simplifying gives an overall complexity of $\mathcal{O}(Q(Qpq + Kd + q^2\log q^2) + K^2d)$\addLG{$q^2 \log q^2$ peut être remplacé par $q^2 + p\log p$ grâce à l'algo quickselect}. Note that $\mathcal{O}(Kq)$ is majored by $\mathcal{O}(Kd)$ since $d \geq q$.
