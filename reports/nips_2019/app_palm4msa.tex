\section{\palm algorithm}
\label{sec:app:palm4msa}

The \palm algorithm~\cite{LeMagoarou2016Flexible} is given in Algorithm~\ref{algo:palm4msa} together with the time complexity of each line, using $A=\min(\nclusters, \datadim)$ and $B=\max(\nclusters, \datadim)$. 
Even more general constraints can be used, the constraint sets $\mathcal{E}_q$ are typically defined as the intersection of the set of unit Frobenius-norm matrices and of a set of sparse matrices.
The unit Frobenius norm is used together with the $\lambda$ factor to avoid a scaling indeterminacy.
Note that to simplify the model presentation, factor $\lambda$ is used internally in \palm and is integrated in factor $\rmS_1$ at the end of the algorithm (Line~\ref{line:palm:postprocess:S1}) so that $\rmS_1$ does not satisfy the unit Frobenius norm in $\mathcal{E}_1$ at the end of the algorithm.
The sparse constraints we used, as in~\cite{LeMagoarou2016Flexible}, consist of trying to have a given number of non-zero coefficients in each row and in each column.
This number of non-zero coefficients is called sparsity level in this paper.
In practice, the projection function at Line~\ref{line:palm:update:S} keeps the largest non-zero coefficients in each row and in each column, which only guarantees
the actual number of non-zero coefficients is at least equal to the sparsity level.




\begin{algorithm}
	\caption{\palm algorithm}
	\label{algo:palm4msa}
	\begin{algorithmic}[1]
		
		\REQUIRE The matrix to factorize $\rmU \in \R^{\nclusters \times \datadim}$, the desired number of factors $\nfactors$, the constraint sets $\mathcal{E}_q$ , $q\in \intint{\nfactors}$ and a stopping criterion (e.g., here, a number of iterations $I$ ).
		
		\STATE $\lambda \leftarrow \norm{S_1}_F$
		\COMMENT{$\bigO{B}$}
		\label{line:palm:init:lambda}
		\STATE $S_1 \leftarrow \frac{1}{\lambda} S_1$
		\COMMENT{$\bigO{B}$}
		\label{line:palm:normalize:S1}
		\FOR {$i \in\intint{I}$ while the stopping criterion is not met}
		\FOR {$q = \nfactors$ down to $1$}
		\STATE  $\rmL_q \leftarrow \prod_{l=1}^{q-1} \rmS_{l}^{(i)}$
		\label{line:palm:L}
		\STATE  $\rmR_q \leftarrow \prod_{l=q+1}^{\nfactors} \rmS_{l}^{(i+1)}$
		\label{line:palm:R}
		\STATE Choose $c > \lambda^2 ||\rmR_q||_2^2 ||\rmL_q||_2^2$
		\COMMENT{$\bigO{A \log A+B}$}
		\label{line:palm:c}
		\STATE $\rmD \leftarrow \rmS_q^i - \frac{1}{c} \lambda \rmL_q^T\left (\lambda\rmL_q \rmS_q^i \rmR_q - \rmU\right )\rmR_q^T$
		\COMMENT{$\bigO{AB\log A}$}
		\label{line:palm:D}
		\STATE $\rmS^{(i+1)}_q \leftarrow P_{\mathcal{E}_q}(\rmD)$
		\COMMENT{$\bigO{A^2\log A}$ or $\bigO{AB\log B}$}
		\label{line:palm:update:S}
		\ENDFOR
		\STATE $\hat \rmU \eqdef \prod_{j=1}^{\nfactors} \rmS_q^{(i+1)}$
		\COMMENT{$\bigO{A^2\log A + AB}$}
		\label{line:palm:U}
		\STATE $\lambda \leftarrow \frac{Trace(\rmU^T\hat\rmU)}{Trace(\hat\rmU^T\hat\rmU)}$
		\COMMENT{$\bigO{AB}$}
		\label{line:palm:update:lambda}
		\ENDFOR
		\STATE $S_1 \leftarrow \lambda S_1$
		\COMMENT{$\bigO{B}$}
		\label{line:palm:postprocess:S1}
		\ENSURE $\left \lbrace \rmS_q : \rmS_q \in \mathcal{E}_q\right \rbrace_{q\in\intint{\nfactors}}$ such that $\prod_{q\in\intint{\nfactors}}\rmS_q \approx \rmU$
		
	\end{algorithmic}
\end{algorithm}

The complexity analysis is proposed under the following assumptions, which are satisfied in the mentioned applications and experiments: the number of factors is $\nfactors=\mathcal{O}\left (\log A\right )$; all but one sparse factors are of shape $A \times A$ and have $\bigO{A}$ non-zero entries while one of them is of shape $A\times B$ or $B\times A$ with $\bigO{B}$ non-zero entries.
In such conditions, the complexity of each line is:
\begin{itemize}
 \item [Lines~\ref{line:palm:init:lambda}-\ref{line:palm:normalize:S1}] Computing these normalization steps is linear in the number of non-zeros coefficients in $\rmS_1$.
 \item [Lines~\ref{line:palm:L}-\ref{line:palm:R}] Fast operators $\rmL$ and $\rmR$ are defined for subsequent use without computing explicitly the product.
 \item [Line~\ref{line:palm:c}] The spectral norm of $\rmL$ and $\rmR$ is obtained via a power method by iteratively applying each operator, benefiting from the fast transform.
 \item [Line~\ref{line:palm:D}] The cost of the gradient step is dominated by the product of sparse matrices.
\item [Line~\ref{line:palm:update:S}] The projection onto a sparse-constraint set takes $\bigO{A^2\log A}$ for all the $A\times A$ matrices and $\bigO{AB\log B}$ for the rectangular matrix at the leftmost or the rightmost position.
 \item [Line~\ref{line:palm:U}] The reconstructed matrix $\hat \rmU$ is computed using $\bigO{\log A}$ products between $A\times A$ sparse matrices, in $\bigO{A^2}$ operations each, and one product with a sparse matrix in $\bigO{AB}$.
 \item [Line~\ref{line:palm:update:lambda}] The numerator and denominator can be computed using a Hadamard product between the matrices followed by a sum over all the entries.
  \item [Line~\ref{line:palm:postprocess:S1}] Computing renormalization step is linear in the number of non-zeros coefficients in $\rmS_1$.
\end{itemize}

Hence, the overal time complexity of \palm is in $\bigO{AB\left (\log^2 A+\log B\right )}$, due to Lines~\ref{line:palm:D} and~\ref{line:palm:update:S}.


