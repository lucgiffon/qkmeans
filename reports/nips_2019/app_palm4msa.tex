\section{\palm algorithm}
\label{sec:app:palm4msa}

The \palm algorithm~\cite{LeMagoarou2016Flexible} is given in Algorithm~\ref{algo:palm4msa} together with the time complexity of each line, using $A=\min(\nclusters, \datadim)$ and $B=\max(\nclusters, \datadim)$. 
Even more general constraints can be used, the constraint sets $\mathcal{E}_q$ are typically defined as the intersection of the set of unit Frobenius-norm matrices and of a set of sparse matrices.
The unit Frobenius norm is used together with the $\lambda$ factor to avoid a scaling indeterminacy.
Note that to simplify the model presentation, factor $\lambda$ is used internally in \palm and is integrated in factor $\rmS_1$ at the end of the algorithm (Line~\ref{line:palm:postprocess:S1}) so that $\rmS_1$ does not satisfy the unit Frobenius norm in $\mathcal{E}_1$ at the end of the algorithm.
The sparse constraints we used, as in~\cite{LeMagoarou2016Flexible}, consist of trying to have a given number of non-zero coefficients in each row and in each column.
This number of non-zero coefficients is called sparsity level in this paper.
In practice, the projection function at Line~\ref{line:palm:update:S} keeps the largest non-zero coefficients in each row and in each column, which only guarantees
the actual number of non-zero coefficients is at least equal to the sparsity level.



%\addVE{Pour que $S_0=\lambda \rmI$ soit à gauche, inverser l'ordre du produit ($S_1, S_2, \ldots, S_{Q-1}$ contre $S_{Q-1}...S_2 S_1$) précédemment: done. The current order is from left to right for indices ($S_1, S_2, \ldots, S_{Q-1}$) while the update is from right to left ($q = \nfactors$ down to $1$). Is it ok?}

\begin{algorithm}
	\caption{\palm algorithm}
	\label{algo:palm4msa}
	\begin{algorithmic}[1]
		
		\REQUIRE The matrix to factorize $\rmU \in \R^{\nclusters \times \datadim}$, the desired number of factors $\nfactors$, the constraint sets $\mathcal{E}_q$ , $q\in \intint{\nfactors}$ and a stopping criterion (e.g., here, a number of iterations $I$ ).
		
%		\ENSURE $\{\rmS_1 \dots \rmS_{\nfactors}\}|\rmS_q \in \mathcal{E}_q$ such that $\prod_{q\in\intint{\nfactors}}\rmS_q \approx \rmU$
		\STATE $\lambda \leftarrow \norm{S_1}_F$
		\COMMENT{$\bigO{B}$}
		\label{line:palm:init:lambda}
		\STATE $S_1 \leftarrow \frac{1}{\lambda} S_1$
		\COMMENT{$\bigO{B}$}
		\label{line:palm:normalize:S1}
		\FOR {$i \in\intint{I}$ while the stopping criterion is not met}
		\FOR {$q = \nfactors$ down to $1$}
%		\FOR {$q = 2$ to $\nfactors$}
%		\STATE  $\rmL_q \leftarrow \prod_{l=q+1}^{\nfactors} \rmS_{l}^{(i)}$
		\STATE  $\rmL_q \leftarrow \prod_{l=1}^{q-1} \rmS_{l}^{(i)}$
%		\COMMENT{$\bigO{1}$}
		\label{line:palm:L}
%		\STATE  $\rmR_q \leftarrow \prod_{l=0}^{q-1} \rmS_{l}^{(i+1)}$
		\STATE  $\rmR_q \leftarrow \prod_{l=q+1}^{\nfactors} \rmS_{l}^{(i+1)}$
%		\COMMENT{$\bigO{1}$}
		\label{line:palm:R}
%		\STATE Choose $c > (\lambda^{(i)})^2 ||\rmR_q||_2^2 ||\rmL_q||_2^2$
		\STATE Choose $c > \lambda^2 ||\rmR_q||_2^2 ||\rmL_q||_2^2$
%		\COMMENT{in $\mathcal{O}\left (A\nfactors+B\right )$}
		\COMMENT{$\bigO{A \log A+B}$}
		\label{line:palm:c}
		\STATE $\rmD \leftarrow \rmS_q^i - \frac{1}{c} \lambda \rmL_q^T\left (\lambda\rmL_q \rmS_q^i \rmR_q - \rmU\right )\rmR_q^T$
%		\COMMENT{in $\mathcal{O}\left (\nclusters\datadim\nfactors\right )$}
		\COMMENT{$\bigO{AB\log A}$}
		\label{line:palm:D}
%		\STATE $\rmD \leftarrow \rmS_q^i - \frac{1}{c} \lambda^{(i)} \rmL_q^T(\lambda^{(i)} \rmL_q \rmS_q^i \rmR_q - \rmU)\rmR_q^T$
		\STATE $\rmS^{(i+1)}_q \leftarrow P_{\mathcal{E}_q}(\rmD)$
%		\COMMENT{in $\mathcal{O}\left (\nclusters\datadim\nfactors\right )$}
		\COMMENT{$\bigO{A^2\log A}$ or $\bigO{AB\log B}$}
		\label{line:palm:update:S}
		\ENDFOR
		\STATE $\hat \rmU \eqdef \prod_{j=1}^{\nfactors} \rmS_q^{(i+1)}$
		\COMMENT{$\bigO{A^2\log A + AB}$}
		\label{line:palm:U}
		\STATE $\lambda \leftarrow \frac{Trace(\rmU^T\hat\rmU)}{Trace(\hat\rmU^T\hat\rmU)}$% \rmI$
%		\STATE $\rmS_1 \leftarrow \frac{Trace(\rmU^T\hat\rmU)}{Trace(\hat\rmU^T\hat\rmU)} \rmI$
%		\COMMENT{in $\mathcal{O}\left (\nclusters\datadim\right )$}
		\COMMENT{$\bigO{AB}$}
		\label{line:palm:update:lambda}
		\ENDFOR
		\STATE $S_1 \leftarrow \lambda S_1$
		\COMMENT{$\bigO{B}$}
		\label{line:palm:postprocess:S1}
		\ENSURE $\left \lbrace \rmS_q : \rmS_q \in \mathcal{E}_q\right \rbrace_{q\in\intint{\nfactors}}$ such that $\prod_{q\in\intint{\nfactors}}\rmS_q \approx \rmU$
		
	\end{algorithmic}
\end{algorithm}

The complexity analysis is proposed under the following assumptions, which are satisfied in the mentioned applications and experiments: the number of factors is $\nfactors=\mathcal{O}\left (\log A\right )$; all but one sparse factors are of shape $A \times A$ and have $\bigO{A}$ non-zero entries while one of them is of shape $A\times B$ or $B\times A$ with $\bigO{B}$ non-zero entries.
In such conditions, the complexity of each line is:
\begin{itemize}
 \item [Lines~\ref{line:palm:init:lambda}-\ref{line:palm:normalize:S1}] Computing these normalization steps is linear in the number of non-zeros coefficients in $\rmS_1$.
 \item [Lines~\ref{line:palm:L}-\ref{line:palm:R}] Fast operators $\rmL$ and $\rmR$ are defined for subsequent use without computing explicitly the product.
% s can be \textit{precomputed} incrementaly for each iteration $i$, involving a total cost of $\mathcal{O}(Qpq)$ operations: for all $j < Q$, $\rmL_j = \rmL_{j+1} \mathcal{S}^i_{j+1}$; for $j = Q$, $\rmL_j = \textbf{Id}$;
% \item [Line 4] The $\rmR$s is computed incrementaly for each iteration $j$: $\rmR_j = \mathcal{S}^{i+1}_{j-1} \rmR_{j-1}$ if $j > 1$; $\rmR_j = \textbf{Id}$ otherwise. This costs an overall $\mathcal{O}(Qpq)$ operations;
 \item [Line~\ref{line:palm:c}] The spectral norm of $\rmL$ and $\rmR$ is obtained via a power method by iteratively applying each operator, benefiting from the fast transform.
 \item [Line~\ref{line:palm:D}] The cost of the gradient step is dominated by the product of sparse matrices.
% \addLG{avec Valentin on avait trouvé O(Kd min \{K, d\}) mais je ne suis plus d'accord} Taking advantage of the decompositions of $\rmL$ and $\rmR$ as products of sparse factors, the time complexity of this line ends up being $\mathcal{O}(Q(Qpq + Kd + q^2\log q^2))$ for a complete iteration: the $\mathcal{O}(Qpq)$ part comes from the various sparse-dense matrix multiplications with $\rmR$ and $\rmL$; the $\mathcal{O}(Kd)$ part comes from the pairwise substraction inside the parentheses and the $\mathcal{O}(q^2 \log q^2)$ part from the projection operator that involves sorting of the inner matrix.\addLG{$q^2 \log q^2$ peut être remplacé par $q^2 + p\log p$ grâce à l'algo quickselect}
\item [Line~\ref{line:palm:update:S}] The projection onto a sparse-constraint set takes $\bigO{A^2\log A}$ for all the $A\times A$ matrices and $\bigO{AB\log B}$ for the rectangular matrix at the leftmost or the rightmost position.
 \item [Line~\ref{line:palm:U}] The reconstructed matrix $\hat \rmU$ is computed using $\bigO{\log A}$ products between $A\times A$ sparse matrices, in $\bigO{A^2}$ operations each, and one product with a sparse matrix in $\bigO{AB}$.
 \item [Line~\ref{line:palm:update:lambda}] The numerator and denominator can be computed using a Hadamard product between the matrices followed by a sum over all the entries.
  \item [Line~\ref{line:palm:postprocess:S1}] Computing renormalization step is linear in the number of non-zeros coefficients in $\rmS_1$.
\end{itemize}

Hence, the overal time complexity of \palm is in $\bigO{AB\left (\log^2 A+\log B\right )}$, due to Lines~\ref{line:palm:D} and~\ref{line:palm:update:S}.

%\todo[inline]{Valentin: I have reintroduced $\lambda$, but just as an internal variable. Is it ok? Check also the compliance of this formulation with the rest of the paper, e.g., with the definition of $\mathcal{E}_q$. Factors are updated from right to left, exact?}
%\todo[inline]{Define $\mathcal{E}_q$ with the sparsity constraint and the Frobenius-norm constraint.}
%\todo[inline]{add details about the hierarchical algorithm? \addVE{Non, pas besoin?}}
%\todo[inline]{The stopping criterion is the number of iterations $I$ here, which is incoherent with section~\ref{sec:uses:settings}.}
