%!TEX root=aaai2020_qmeans.tex

\section{Preliminaries}
\label{sec:background}
We briefly review the basics of \kmeans and give background on learning fast transforms.
To  assist  the  reading,  we  list  the notations used in the paper in Table~\ref{tab:notation}.



\input{notations.tex}





\subsection{\kmeans}
\label{sec:kmeans}
The \kmeans problem is that of partitioning a set $\rmX=\{\rvx_1,\ldots,\rvx_N\}$ of $N$  vectors $\rvx_n\in\R^{\datadim}$ into a predefined number $\nclusters$ of clusters
with the aim of minimizing the distance between each $\rvx_n$ to the center $\rvu_k\in\R^{D}$
of the cluster $k$ it belongs to ---center $\rvu_k$ is the
 mean vector of the points assigned to cluster $k$.
The optimization problem of \kmeans is
\begin{equation}
\label{eq:kmean_problem}
    \argmin_{\rmU, \rvt} \sum_{k\in\intint{\nclusters}} \sum_{\substack{n\\t_n = k}} \|\rvx_{n} -\rvu_{k}\|^2,
\end{equation}
where $\rmU=\{\rvu_1,\ldots,\rvu_K\}$ is the set of centers and $\rvt \in  \intint{\nclusters}^{\nexamples}$ is the assignment vector that puts $\rvx_n$ in cluster $k$
if $t_n=k$.


\paragraph{Lloyd's algorithm (a.k.a. \kmeans algorithm).} The most popular procedure to (approximately) 
solve the \kmeans problem is Lloyd's algorithm, which is referred to as the \kmeans algorithm in the litterature as well as in this paper.
It alternates between
i) an assignment step that decides the current cluster to which each point $\rvx_n$
belongs and ii) a reestimation step which refines the cluster centers.
%In little more detail, the algorithm starts with 
After an initialization of the set $\rmU^{(0)}$ of $\nclusters$
 cluster centers, the algorithm proceeds as follows: at iteration $\tau$,
  the assignments are updated as
\begin{align}
%\begin{split}
\label{eq:assignment_problem_kmeans}
 t_n^{(\tau)} &\leftarrow \argmin_{k \in \intint{\nclusters]}} \left\|\rvx_{n} - \rvu_{k}^{(\tau-1)}\right\|_2^2,\quad \forall n\in\intint{N},\\
%\\
%&= \argmin_{k \in \intint{\nclusters}} \left\|\rvu_{k}^{(\tau-1)}\right\|_2^2 - 2 \left\langle\rvu_{k}^{(\tau-1)}, \rvx_{n}\right\rangle,
%\end{split}
%\end{align}
\intertext{and the reestimation of the cluster centers is done as}
%\begin{align}
\label{eq:center_update}
\rvu^{(\tau)}_k &\leftarrow \hat{\rvx}_k(\rvt^{(\tau)}) \eqdef \frac{1}{n_k^{(\tau)}} \sum_{\substack{n\\t^{(\tau)}_n= k}} {\rvx_{n}},\quad \forall k\in\intint{K} 
\end{align}
where $n_k^{(\tau)}\eqdef |\{n: t^{(\tau)}_n=k\}|$ is the number of points in cluster $k$
at time $\tau$ and $\hat{\rvx}_k(\rvt)$ is the mean vector of the elements of cluster $k$ according to assignment $\rvt$. 

\paragraph{Complexity of Lloyd's algorithm.} The assignment step \eqref{eq:assignment_problem_kmeans} costs $\mathcal{O}(\nexamples\datadim\nclusters)$ operations while the update of the centers~\eqref{eq:center_update} costs $\mathcal{O}\left (\nexamples\datadim\right )$ operations. Hence, the bottleneck of the overall time complexity $\mathcal{O}(\nexamples\datadim\nclusters)$ stems from the assignment step. Once the clusters have been defined, assigning $\nexamples'$ new points to these clusters is performed via \eqref{eq:assignment_problem_kmeans} at the cost of $\mathcal{O}\left (\nexamples'\datadim\nclusters \right )$ operations.

SAY SOMEWHERE: From now on, with some abuse of notation, we will use the term
\kmeans algorithm to refert to Lloyd's algorithm. 

Our main contribution rests on the idea that \eqref{eq:assignment_problem_kmeans} may be computed more efficiently by approximating $\rmU$ as a {\em fast operator}.

WATCH: operator vs transform... 

\subsection{Fast Transforms as Products of Sparse Matrices}
\label{sec:palm4msa}

\paragraph{Structured linear operators as products of sparse matrices.}
The popularity of some linear operators from $\R^{M}$ to $\R^{M}$ (with $M<\infty$)
 like Fourier or Hadamard transforms comes from both their mathematical 
 properties and their ability to compute the mapping of some input $\rvx\in\R^M$ with efficiency, typically in $\mathcal{O}\left (M\log M\right )$ in lieu of  
  $\mathcal{O}\left (M^2\right)$ operations.
The core feature of the related fast algorithms is that the matrix $\rmU\in\sR^{M\times M}$ of such 
linear operators can be written as the product $\rmU=\Pi_{q\in\intint{\nfactors}}\rmS_q$ 
of $\nfactors=\bigO{\left( \log M \right)}$ sparse 
matrices $\rmS_q$ and $\left \|\rmS_q\right \|_0=\mathcal{O}\left( M \right)$ non-zero 
coefficients per factor \cite{LeMagoarou2016Flexible,Morgenstern1975Linear}:
for any vector $\rvx\in\sR^M$, $\rmU\rvx$ can thus be computed as $\mathcal{O}\left (\log M\right )$ products $\rmS_0 \left (\rmS_1 \left (\cdots \left (\rmS_{Q-1}\rvx\right )\right )\right )$ between a sparse matrix and a vector, the cost of each product being $\mathcal{O}\left (M\right )$, amounting to a $\mathcal{O}(M \log M)$ time complexity.

\paragraph{Learning a computationally-efficient decomposition approximating an arbitrary operator.} When the linear operator $\rmU$ is an arbitrary matrix, one may approximate it with such a sparse-product structure by learning the factors $\left \lbrace\rmS_q\right \rbrace_{q\in\intint{Q}}$ in order to benefit from a fast algorithm.
A recent contribution~\cite{LeMagoarou2016Flexible} has proposed algorithmic strategies to learn such a factorization. Based on the proximal alternating linearized minimization (\texttt{PALM}) algorithm~\cite{bolte2014proximal}, the \texttt{PALM} for Multi-layer Sparse Approximation (\palm) algorithm~\cite{LeMagoarou2016Flexible} aims at approximating a matrix $\rmU\in\sR^{\nclusters\times\datadim}$ as a product of sparse matrices by solving
\begin{align}
\label{eq:palm4msa}
\min_{\left \lbrace\rmS_q\right \rbrace_{q\in\intint{Q}}} \left \|\rmU -  \prod_{q\in\intint{\nfactors}}{\rmS_q}\right \|_F^2 + \sum_{q\in\intint{\nfactors}} \delta_{\mathcal{E}_q}(\rmS_q)
\end{align}
where, for each $q\in\intint{Q}$, $\delta_{\mathcal{E}_q}(\rmS_q)=0$ 
if $\rmS_q \in \mathcal{E}_q$ and $\delta_{\mathcal{E}_q}(\rmS_q)=+\infty$ otherwise, $\mathcal{E}_q$ being a constraint set that typically imposes a sparsity structure on its elements, as well as a scaling constraint. 
Although this problem is non-convex and the computation of a global optimum cannot be
ascertained, the \palm algorithm is able to find %good 
local minima with convergence guarantees. 
(In addition to the reference papers, details on \palm are given in the appendix.)



