%!TEX root=neurips2019_qmeans.tex
\section{Preliminaries}
\label{sec:background}
We briefly review the basics of \kmeans and give background on learning fast transforms.
To  assist  the  reading,  we  list  the notations used in the paper in Table~\ref{tab:notation}.



\input{notations.tex}





\subsection{\kmeans}
\label{sec:kmeans}
The \kmeans algorithm is used to partition a set $\rmX=\{\rvx_1,\ldots,\rvx_N\}$ of $N$  vectors $\rvx_n\in\R^{\datadim}$ into a predefined number $\nclusters$ of clusters
with the aim of minimizing the distance between each $\rvx_n$ to the center $\rvu_k\in\R^{D}$
of the cluster $k$ it belongs to ---the center $\rvu_k$ of cluster $k$ is the
 mean vector of the points assigned to cluster $k$.
\kmeans attempts to solve
\begin{equation}
\label{eq:kmean_problem}
    \argmin_{\rmU, \rvt} \sum_{k\in\intint{\nclusters}} \sum_{n: t_n = k} \|\rvx_{n} -\rvu_{k}\|^2,
\end{equation}
where $\rmU=\{\rvu_1,\ldots,\rvu_K\}$ is the set of cluster centers and $\rvt \in  \intint{\nclusters}^{\nexamples}$ is the assignment vector that puts $\rvx_n$ in cluster $k$
if $t_n=k$.


\paragraph{Lloyd's algorithm.} The most popular procedure to (approximately) 
solve the \kmeans problem is the iterative Lloyds algorithm, which alternates
i) an assignment step that decides the current cluster to which each point $\rvx_n$
belongs and ii) a reestimation step which refines the clusters and their centers.
In little more detail, the algorithm starts with an initialized set of $\nclusters$
 cluster centers $\rmU^{(0)}$ and proceeds as follows: at iteration $\tau$,
  the assignments are updated as
\begin{align}
\label{eq:assignment_problem_kmeans}
\forall n\in\intint{N}, t_n^{(\tau)} \leftarrow \argmin_{k \in \intint{\nclusters]}} \left\|\rvx_{n} - \rvu_{k}^{(\tau-1)}\right\|_2^2 = \argmin_{k \in \intint{\nclusters}} \left\|\rvu_{k}^{(\tau-1)}\right\|_2^2 - 2 \left\langle\rvu_{k}^{(\tau-1)}, \rvx_{n}\right\rangle,
\end{align}
 the reestimation of the cluster centers is performed as
\begin{align}
\label{eq:center_update}
\forall k\in\intint{K}, \rvu^{(\tau)}_k \leftarrow \hat{\rvx}_k(\rvt^{(\tau)}) \eqdef \frac{1}{n_k^{(\tau)}} \sum_{n: t^{(\tau)}_n= k} {\rvx_{n}}
\end{align}
where $n_k^{(\tau)}\eqdef |\{n: t^{(\tau)}_n=k\}|$ is the number of points in cluster $k$
at time $\tau$ and $\hat{\rvx}_k(\rvt)$ is the mean vector of the elements of cluster $k$ according to assignment $\rvt$. 

\paragraph{Complexity of Lloyd's algorithm.} The assignment step \eqref{eq:assignment_problem_kmeans} costs $\mathcal{O}(\nexamples\datadim\nclusters)$ operations while the update of the centers~\eqref{eq:center_update} costs $\mathcal{O}\left (\nexamples\datadim\right )$ operations. Hence, the bottleneck of the overall time complexity $\mathcal{O}(\nexamples\datadim\nclusters)$ stems from the assignment step. Once the clusters have been defined, assigning $\nexamples'$ new points to these clusters is performed via \eqref{eq:assignment_problem_kmeans} at the cost of $\mathcal{O}\left (\nexamples'\datadim\nclusters \right )$ operations.

The main contribution in this paper relies on the idea that \eqref{eq:assignment_problem_kmeans} may be computed more efficiently by approximating $\rmU$ as a fast operator.


\subsection{Learning Fast Transforms as the Product of Sparse Matrices}
\label{sec:palm4msa}

\paragraph{Structured linear operators as products of sparse matrices.}
The popularity of some linear operators from $\R^{M}$ to $\R^{M}$ (with $M<\infty$)
 like Fourier or Hadamard transforms comes from both their mathematical 
 properties and their ability to compute the mapping of some input $\rvx\in\R^M$ with efficiency, typically in $\mathcal{O}\left (M\log\left (M\right )\right )$ rather than 
 in $\mathcal{O}\left (M^2\right)$ operations .
The main idea of the related fast algorithms is that the matrix $\rmU\in\sR^{M\times M}$ characterizing such linear operators can be written as the product $\rmU=\Pi_{q\in\intint{\nfactors}}\rmS_q$ of $\nfactors$ sparse matrices $\rmS_q$, with $Q=\mathcal{O}\left (\log M\right )$ factors and $\left \|\rmS_q\right \|_0=\mathcal{O}\left (M\right )$ non-zero coefficients per factor \cite{LeMagoarou2016Flexible,Morgenstern1975Linear}:
for any vector $\rvx\in\sR^M$, $\rmU\rvx$ can thus be computed as $\mathcal{O}\left (\log M\right )$ products $\rmS_0 \left (\rmS_1 \left (\ldots \left (\rmS_{Q-1}\rvx\right )\right )\right )$ between a sparse matrix and a vector, the cost of each product being $\mathcal{O}\left (M\right )$. This gives a $\mathcal{O}(M \log M)$ time complexity for computing $\rmU\rvx$ in that case.

\paragraph{Learning a computationally-efficient decomposition approximating an arbitrary operator.} When the linear operator $\rmU$ is an arbitrary matrix, one may approximate it with such a sparse-product structure by learning the factors $\left \lbrace\rmS_q\right \rbrace_{q\in\intint{Q}}$ in order to benefit from a fast algorithm.
A recent contribution~\cite{LeMagoarou2016Flexible} has proposed algorithmic strategies to learn such a factorization. Based on the proximal alternating linearized minimization (\texttt{PALM}) algorithm~\cite{bolte2014proximal}, the \texttt{PALM} for Multi-layer Sparse Approximation (\palm) algorithm~\cite{LeMagoarou2016Flexible} aims at approximating a matrix $\rmU\in\sR^{\nclusters\times\datadim}$ as a product of sparse matrices by solving
\begin{align}
\label{eq:palm4msa}
\min_{\left \lbrace\rmS_q\right \rbrace_{q\in\intint{Q}}} \left \|\rmU -  \prod_{q\in\intint{\nfactors}}{\rmS_q}\right \|_F^2 + \sum_{q\in\intint{\nfactors}} \delta_{\mathcal{E}_q}(\rmS_q)
\end{align}
where, for each $q\in\intint{Q}$, $\delta_{\mathcal{E}_q}(\rmS_q)=0$ 
if $\rmS_q \in \mathcal{E}_q$ and $\delta_{\mathcal{E}_q}(\rmS_q)=+\infty$ otherwise, $\mathcal{E}_q$ being a constraint set that typically impose a sparsity structure on its elements, as well as a scaling constraint. The \palm algorithm and more related details are given in Appendix~\ref{sec:app:palm4msa}.


Although this problem is non-convex and the computation of a global optimum cannot be
ascertained, the \palm algorithm is able to find good local minima with convergence guarantees. 


