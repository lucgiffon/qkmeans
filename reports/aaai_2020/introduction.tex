%!TEX root=aaai2020_qmeans.tex
\section{Introduction}

\kmeans is one of the most popular clustering algorithms~\cite{hartigan1979algorithm,jain2010data} and it can be used beyond clustering, for tasks such as indexing, data compression,  nearest-neighbor search and prediction, and local network community detection~\cite{muja2014scalable,van2016local}. \kmeans is also a pivotal process to increase the speed and the accuracy of learning procedures for, e.g.,  for kernel machines~\cite{si2016computationally} and RBF networks~\cite{que2016back}, when combined with the Nystr√∂m approximation.
%
The  conventional  \kmeans  algorithm, i.e.  Lloyd's algorithm, has  a $\bigO{\nexamples\nclusters\datadim}$  complexity  per iteration, where $\nexamples$ is the number of data points, $\nclusters$ the number of clusters and $\datadim$ is the dimension of the data points.
However, the larger the number of clusters, the more iterations are required to converge~\cite{arthur2006slow}.
%
As data dimensionality and data sample size grow, it is critical to have at hand cost-effective 
alternatives to the computationally expensive conventional \kmeans. 
Known strategies to alleviate the computational issues in \kmeans may rely on batch-, sparsity- and randomization-based methods~\cite{Sculley2010Web,boutsidis2014randomized,shen2017compressed,liu2017sparse}.

Fast transforms have recently received increased attention in machine learning community as they can be used  to speed up random projections~\cite{le2013fastfood,gittens2016revisiting} and to improve landmark-based approximations~\cite{si2016computationally}.
%
These works primarily focused on fast transforms such as Fourier and Hadamard transforms, which are fixed before the learning begins. A question is whether one can go beyond that and learn the fast transform from data. 
%
Recently, \citet{LeMagoarou2016Flexible} introduced a sparse matrix approximation scheme aimed  at  reducing the  complexity  of  applying  linear  operators  in  high  dimension by   approximately   factorizing   the   corresponding   matrix   into few   sparse   factors. One interesting observation is that fast transforms, such as the  Hadamard  transform  and  the  Discrete  Cosine  transform, can be exactly or approximately decomposed as a product of sparse matrices.
%
In this paper, we take this idea further and investigate attractive and computationally less costly implementations of the \kmeans algorithm by learning a fast transform from data.
%
Specifically, we make the following contributions:
\begin{itemize}
	\item we introduce \texttt{QuicK-means}, a fast extension of \kmeans that rests on the idea of expressing the matrix of the $K$ centroids as a product of sparse matrices, a feat made possible by recent results devoted to find approximations of matrices as a product of sparse factors,
	\item we show that each update step in one iteration of our algorithm  reduces the overall objective, which is enough to guarantee the convergence of \texttt{QuicK-means},
	\item we perform a complexity analysis of our algorithm, showing that the computational gain in \texttt{QuicK-means}  has a direct impact in the assignment process of a point to a cluster, meaning that it is not only tangible at prediction time, but also at training time,
	\item we provide an empirical evaluation of \texttt{QuicK-means}  performance which demonstrates its effectiveness on different datasets in the contexts of clustering and kernel Nystr\"om approximation.
\end{itemize}



