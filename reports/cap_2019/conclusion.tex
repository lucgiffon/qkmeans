\section{Conclusion}
\label{sec:conclusion}

In this paper, we proposed a new algorithm with convergence proof, that allows to learn a matrix of K-means center-points with sparse factorization constraint. We provide complexity analysis showing, that this particular matrix may speed up further machine learning algorithms that would usually make use of the standard K-means center-point matrix. In particular, we show that this algorithm could be used in the context of the Nyström approximation.

This paper does not contain any experimental results as it describes ongoing work. In the following, %next subsection (Section \ref{sec:foreseen_experiments}), 
we discuss foreseen experiments that would aim at illustrating the speed gain when using our method, while not reducing the overall accuracy in machine learning settings.
%
%Last but not least, we discuss in the Section \ref{sec:discussion} 
We also discuss a broader scope of application of our algorithm and some possible theoretical advantages.

\subsection{Foreseen experiments}
\label{sec:foreseen_experiments}

Our algorithm will be evaluated on the same metrics than \cite{si2016computationally} on the Nyström approximation, namely the reconstruction error of the kernel matrix and the accuracy error in subsequent machine learning experiments. Those errors will be considered with respect to the computation speed. The considered baseline will be Nyström with simple K-means selected landmark points and, obviously, the efficient Nyström algorithm proposed in \cite{si2016computationally}.

\subsection{Discussion}
\label{sec:discussion}

The algorithm we propose, \textit{Q-means}, could be applied to any other method that uses the K-means algorithm in its initialization. The K-means Nyström method is only one instance of such algorithm but we can already think of other examples, such as some nearest neighbour search algorithm based on the K-means clustering of the input space \cite{wang2011fast}.

Finally, the sparse factorization constraint for the K-means center point matrix may play the role of a regularization, with parameter being the number of values in each factor. 
We wish to investigate also this property more thoroughly from both theoretical and experimental point of view.
%This is just an intuition for now and this must be investigated more thoroughly from both the theoretical and experimental point of view.

%\begin{itemize}
% \item rbf networks
% \item hierarchical nearest neighbours
%\end{itemize}
