\section{Contribution}
\label{sec:contribution}
\subsection{Q-means}

We propose an extension of the K-means algorithm in which the matrix of center-points $\rmU$ is constrained to be expressed as a product of sparse matrices $\mathcal{S}_j: j = 1 \ldots Q$. From Equation \ref{eq:kmean_problem} and Equation \ref{eq:problem_gribon} we can write a new K-means optimisation problem with sparse factorization constraint which we call \textit{Q-means}:

\begin{equation}
\begin{split}
\label{eq:qmean_problem}
 \argmin_{\{\mathcal{S}_1 \dots \mathcal{S}_Q, \lambda\}, \rvt} & g(\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q \}, \lambda, \rvt)\\
    =\argmin_{\{\mathcal{S}_1 \dots \mathcal{S}_Q, \lambda\}, \rvt} & \sum_{k=1}^{K} \left( \sum_{j: \rvt_j = k} ||\rmX_j -\rmU_k||^2 \right) + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j) \\
    & s.t. ~ \rmU = \lambda \prod_{j=1}^{Q}{\mathcal{S}_j}
\end{split}
\end{equation}.

This problem can be solved using Algorithm \ref{algo:qmeans} which is a simple extension of the K-means algorithm (Algorithm \ref{algo:kmeans}) and is guaranteed to converge. To show this convergence, we need to show that each update step in one iteration $\tau$ of the algorithm actually reduces the overall objective. To this end, we start by re-writting the objective at a given time-step $\tau$:

\begin{equation}
\begin{split}
\label{eq:qmean_problem_2}
    g(&\{ \mathcal{S}_1^{(\tau)}, \ldots,\mathcal{S}_Q^{(\tau)} \}, \lambda^{(\tau)}, \rvt^{(\tau)})\\
    = & \sum_{k=1}^{K} \left( \sum_{j: \rvt^{(\tau)}_j = k} ||\rmX_j - \rmU^{(\tau)}_k||^2 \right) + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j^{(\tau)})\\
    & s.t. ~ \rmU = \lambda^{(\tau)} \prod_{j=1}^{Q}{\mathcal{S}_j^{(\tau)}}
\end{split}
\end{equation}.

Then we assess wether or not this objective diminish at each time-step in Algorithm \ref{algo:qmeans}.

\paragraph{Assignment step (Line \ref{line:qmeans:assignment})} For a fixed $\rmU^{(\tau-1)}$ the new indicator vector $\rvt^{(\tau)}$ is defined such as:

\begin{equation}
 \rvt^{(\tau)}_i = \argmin_{k \in [\![K]\!]} ||\rmX_i - \rmU^{(\tau-1)}||_2^2
\end{equation}

for any $i \in [\![n]\!]$. This step is exactly identical in the K-means algorithm (Algorithm \ref{algo:kmeans}) and is clearly minimizing the objective function \textit{w.r.t.} to vector $\rvt$.

\paragraph{Centroids computation step (Line \ref{line:qmeans:startkmeans} to \ref{line:qmeans:endkmeans})} For a fixed $\rvt^{(\tau)}$, the new sparsely-factorized centroids are solution of the following subproblem:

\begin{equation}
\label{eq:qmeans_problem_t_fixed}
\begin{split}
 \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & g(\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda, \rvt^{(\tau)}) \\
 = \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} &\sum_{k=1}^{K} \left( \sum_{j: \rvt^{(\tau)}_j = k} ||\rmX_j - \rmU_k||^2_2 \right) + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j)  \\
 = \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & ||\mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~(\hat{\rmX}^{(\tau)} - \rmU)||_{\mathcal{F}} ^ 2  \\
 &+ \sum_{k=1}^{K} c_k^{(\tau)} + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j)\\
 & s.t. ~ \rmU = \lambda \prod_{j=1}^{Q}{\mathcal{S}_j}
\end{split} 
\end{equation}

where :

\begin{itemize}
 \item $\sqrt{\rvn^{(\tau)}} \in {\R^{K}}$ is the pair-wise square root of the vector indicating the number of observation in each cluster at step $\tau$: $\rvn_k^{(\tau)} = |\{i: \rvt^\tau_i = k\}|$;
 \item $\mathcal{D}_\rvv \in \R^{K \times K}$ refers to a diagonal matrix with entries in the diagonal from a vector $\rvv$;
 \item $\hat{\rmX}^{(\tau)} \in \R^{K \times d}$ refers to the real centroid matrix obtained at step $\tau$ \textit{w.r.t} the indicator vector at this step $\rvt^{(\tau)}$: $\hat{\rmX}^{(\tau)}_k = \frac{1}{\rvn_k}\sum_{j:\rvt^{(\tau)}_j = k} {\rmX_j}$. When $\rvt^{(\tau)}$ is fixed, this is constant.
 \item $c_k^{(\tau)} = \sum_{j: \rvt^{(\tau)}_j = k}^{}||\rmX_j - \hat{\rmX}_k^{(\tau)}||$ is constant \textit{w.r.t} $\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}$ and $\lambda$.
\end{itemize}

Again,  the minimization of the overall objective $g$ from Equation \ref{eq:qmean_problem_2} is clear since the $\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}$ and $\lambda$ are precisely chosen to minimize $g$.

Note that the formulation of the problem in Equation \ref{eq:qmeans_problem_t_fixed} shows the connection between the K-means and \textit{Hierarchical PALM4LED} objectives, which allow to combine them without trouble. Indeed, we can set

\begin{equation*}
\rmA^{(\tau)} = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\hat{\rmX}^{(\tau)}
\end{equation*}
and
\begin{equation*}
\rmB^{(\tau)} = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\rmU = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\lambda \prod_{j=1}^{Q}{\mathcal{S}_j} = \lambda \prod_{j=0}^{Q}{\mathcal{S}_j}
\end{equation*}

with $\mathcal{S}_0$ fixed and equal to $\mathcal{D}_{\sqrt{\rvn^{(\tau)}}}$. The Equation \ref{eq:qmeans_problem_t_fixed} can then be rewritten 

\begin{equation}
\begin{split}
 \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & ||\rmA^{(\tau)} - \rmB^{(\tau)}||_{\mathcal{F}} ^ 2  +  \sum_{j=0}^{Q} \delta_j(\mathcal{S}_j)\\
 s.t. &~ \rmB^{(\tau)} = \lambda \prod_{j=0}^{Q}{\mathcal{S}_j}
\end{split}
\end{equation}

Since \textit{Hierarchical PALM4LED} successivly update the $\mathcal{S}_j$s independently and in an alternating fashion, we can still use \textit{PALM4LED} in to solve this problem with the $\mathcal{S}_0$ fixed.


\begin{algorithm}
\caption{Q-means algorithm}
\label{algo:qmeans}
\begin{algorithmic}[1]


\REQUIRE $\rmX \in \R^{n \times d}$, $K$, $\{\mathcal{S}_1 \dots \mathcal{S}_{Q}\}|\mathcal{S}_j \in \mathcal{E}_j$
\ENSURE $\{\mathcal{S}_1 \dots \mathcal{S}_{Q}\}|\mathcal{S}_j \in \mathcal{E}_j$ such that $\prod_{j=1}^{Q}\mathcal{S}_j \approx \rmU$ the K means of $n$ $d$-dimensional samples
\STATE $\tau \leftarrow 0$
\REPEAT
\STATE $\tau \leftarrow \tau + 1$
\STATE $\rvt^{(\tau)} \leftarrow \argmin_{\rvt \in [\![K]\!]^n} \sum_{i=1}^{n} {||\rmX_i - \rmU^{(\tau -1)}_{\rvt(i)}||_2^2}$
\label{line:qmeans:assignment}
\FORALL {$k \in [\![K]\!]$}
\label{line:qmeans:startkmeans}
\STATE $n_k^{(\tau)} \leftarrow |\{i: \rvt^{(\tau)}_i=k\}|$
\STATE $\hat{\rmX}^{(\tau)}_k \leftarrow \frac{1}{n^\tau_k} \sum_{i: \rvt^\tau_i = k} {\rvx_i}$
\ENDFOR
\label{line:qmeans:endkmeans}
\STATE $\rmA^{(\tau)} \leftarrow \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\hat{\rmX}^{(\tau)} $
\STATE $\{\mathcal{S}^{(\tau)}_1 \dots \mathcal{S}^{(\tau)}_{Q}\}, \lambda^{(\tau)} \leftarrow \argmin_{\{\mathcal{S}_1 \dots \mathcal{S}_Q, \lambda\}} ||\rmA^{(\tau)} - ~\lambda\prod_{j=0}^{Q}{\mathcal{S}_j}||_\mathcal{F}^2 + \sum_{j=0}^{Q} \delta_j(\mathcal{S}_j)$
\STATE $\rmU^{(\tau)}_k \leftarrow \lambda^{(\tau)} \prod_{j=1}^{Q}{\mathcal{S}_j^{(\tau)}}$

\UNTIL{stop criterion}
\end{algorithmic}
\end{algorithm}


The factorization of $\rmU$ could then be used in ulterior algorithm that involve a matrix-vector multiplication with $\rmU$: typically any algorithm involving the assignment of some data points to one of the cluster (Equation \ref{eq:assignment_problem_kmeans}). Such applications of our proposed algorithm are discussed in Section \ref{sec:uses}.

\subsection{Complexity}
\paragraph{Complexity of a multiplication of dense matrices.}
If $\rmA$ is an $M\times K $ matrix and $\rmB$ is an $K \times N$ matrix, then computing $\rmA \rmB$ can be done in $\mathcal{O}\left (MKN\right )$ operations.

\paragraph{Complexity of a multiplication with a sparse matrix.}
If $\rmA$ is a sparse matrix with $a$ non-zero entries and $\rmB$ is a dense matrix with $N$ columns, then computing $\rmA \rmB$ can be done in $\mathcal{O}\left (aN\right )$ operations.
If $\rmB$ is also sparse, with $b$ non-zero entries, then the bound is $\mathcal{O}\left ( \min\left ( a \min\left (b, N \right ), b \min\left (a, M\right ) \right ) \right )$ where $M$ is the number of rows in $\rmA$.
This is a naive upper bound for sparse matrices, some tighter bound may be found.

\paragraph{Complexity of algorithm Palm4LED.}
to be completed

\paragraph{Complexity of algorithm Hierarchical PALM4LED.}
to be completed

\paragraph{Complexity of Kmeans (algorithm~\ref{algo:kmeans}).}
Each iteration takes $\mathcal{O}\left (ndk\right )$
\begin{itemize}
\item Assignment, line~\ref{line:kmeans:assignment}: $\mathcal{O}\left (ndk\right )$\\
it is dominated by the computation of $\rmU \rmX^T$ using $\left \|\rvx-\rvu\right \|_2^2=\left \|\rvx\right \|_2^2+\left \|\rvu\right \|_2^2-2\left <\rvx, \rvu\right >$
\item Computing size of cluster, line~\ref{line:kmeans:count}: $\mathcal{O}\left (n \right )$\\
it consists in one pass over $\rvt$.
\item Updating the centroids, line~\ref{line:kmeans:compute_means}: $\mathcal{O}\left (nd\right )$\\
since each example is summed once.
\end{itemize}

\paragraph{Complexity of Q-means (algorithm~\ref{algo:qmeans}).}
