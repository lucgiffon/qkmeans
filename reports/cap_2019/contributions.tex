\section{Contribution}
\label{sec:contribution}
\subsection{Q-means}

We propose an extension of the K-means algorithm in which the matrix of center-points $\rmU$ is constrained to be expressed as a product of sparse matrices $\mathcal{S}_j: j = 1 \ldots Q$. From Equation \ref{eq:kmean_problem} and Equation \ref{eq:problem_gribon} we can write a new K-means optimisation problem with sparse factorization constraint which we call \textit{Q-means}:
%
\begin{equation}
\begin{split}
\label{eq:qmean_problem}
 \argmin_{\{\mathcal{S}_1 \dots \mathcal{S}_Q, \lambda\}, \rvt} & g(\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q \}, \lambda, \rvt)\\
    =\argmin_{\{\mathcal{S}_1 \dots \mathcal{S}_Q, \lambda\}, \rvt} & \sum_{k=1}^{K} \left( \sum_{j: \rvt_j = k} ||\rmX_j -\rmU_k||^2 \right) + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j) \\
    & s.t. ~ \rmU = \lambda \prod_{j=1}^{Q}{\mathcal{S}_j}
\end{split}
\end{equation}.
%
This problem can be solved using Algorithm \ref{algo:qmeans} which is a simple extension of the K-means algorithm (Algorithm \ref{algo:kmeans}) and is guaranteed to converge. To show this convergence, we need to show that each update step in one iteration $\tau$ of the algorithm actually reduces the overall objective. To this end, we start by re-writing the objective at a given time-step $\tau$:
%
\begin{equation}
\begin{split}
\label{eq:qmean_problem_2}
    g(&\{ \mathcal{S}_1^{(\tau)}, \ldots,\mathcal{S}_Q^{(\tau)} \}, \lambda^{(\tau)}, \rvt^{(\tau)})\\
    = & \sum_{k=1}^{K} \left( \sum_{j: \rvt^{(\tau)}_j = k} ||\rmX_j - \rmU^{(\tau)}_k||^2 \right) + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j^{(\tau)})\\
    & s.t. ~ \rmU = \lambda^{(\tau)} \prod_{j=1}^{Q}{\mathcal{S}_j^{(\tau)}}
\end{split}
\end{equation}.
%
We then assess whether or not this objective diminishes at each time-step in Algorithm \ref{algo:qmeans}.

\paragraph{Assignment step (Line \ref{line:qmeans:assignment})} For a fixed $\rmU^{(\tau-1)}$ the new indicator vector $\rvt^{(\tau)}$ is defined as:
%
\begin{equation}
 \rvt^{(\tau)}_i = \argmin_{k \in [\![K]\!]} ||\rmX_i - \rmU^{(\tau-1)}||_2^2
\end{equation}
%
for any $i \in [\![n]\!]$. This step is exactly identical in the K-means algorithm (Algorithm \ref{algo:kmeans}) and is clearly minimizing the objective function \textit{w.r.t.} to vector $\rvt$.

\paragraph{Centroids computation step (Line \ref{line:qmeans:startkmeans} to \ref{line:qmeans:endkmeans})} For a fixed $\rvt^{(\tau)}$, the new sparsely-factorized centroids are solutions of the following subproblem:
%
\begin{equation}
\label{eq:qmeans_problem_t_fixed}
\begin{split}
 \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & g(\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda, \rvt^{(\tau)}) \\
 = \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} &\sum_{k=1}^{K} \left( \sum_{j: \rvt^{(\tau)}_j = k} ||\rmX_j - \rmU_k||^2_2 \right) + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j)  \\
 = \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & ||\mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~(\hat{\rmX}^{(\tau)} - \rmU)||_{\mathcal{F}} ^ 2  \\
 &+ \sum_{k=1}^{K} c_k^{(\tau)} + \sum_{j=1}^{Q} \delta_j(\mathcal{S}_j)\\
 & s.t. ~ \rmU = \lambda \prod_{j=1}^{Q}{\mathcal{S}_j}
\end{split} 
\end{equation}
%
where :
%
\begin{itemize}
 \item $\sqrt{\rvn^{(\tau)}} \in {\R^{K}}$ is the pair-wise square root of the vector indicating the number of observations in each cluster at step $\tau$: $\rvn_k^{(\tau)} = |\{i: \rvt^\tau_i = k\}|$;
 \item $\mathcal{D}_\rvv \in \R^{K \times K}$ refers to a diagonal matrix with entries in the diagonal from a vector $\rvv$;
 \item $\hat{\rmX}^{(\tau)} \in \R^{K \times d}$ refers to the real centroid matrix obtained at step $\tau$ \textit{w.r.t} the indicator vector at this step $\rvt^{(\tau)}$: $\hat{\rmX}^{(\tau)}_k = \frac{1}{\rvn_k}\sum_{j:\rvt^{(\tau)}_j = k} {\rmX_j}$. When $\rvt^{(\tau)}$ is fixed, this is constant.
 \item $c_k^{(\tau)} = \sum_{j: \rvt^{(\tau)}_j = k}^{}||\rmX_j - \hat{\rmX}_k^{(\tau)}||$ is constant \textit{w.r.t} $\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}$ and $\lambda$.
\end{itemize}

Again,  the minimization of the overall objective $g$ from Equation \ref{eq:qmean_problem_2} is clear since the $\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}$ and $\lambda$ are precisely chosen to minimize $g$.

Note that the formulation of the problem in Equation \ref{eq:qmeans_problem_t_fixed} shows the connection between the K-means and \textit{Hierarchical PALM4LED} objectives, which allows us to combine them without trouble. Indeed, we can set
%
\begin{equation*}
\rmA^{(\tau)} = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\hat{\rmX}^{(\tau)}
\end{equation*}
and
\begin{equation*}
\rmB^{(\tau)} = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\rmU = \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\lambda \prod_{j=1}^{Q}{\mathcal{S}_j} = \lambda \prod_{j=0}^{Q}{\mathcal{S}_j}
\end{equation*}
%
with $\mathcal{S}_0$ fixed and equal to $\mathcal{D}_{\sqrt{\rvn^{(\tau)}}}$. The Equation \ref{eq:qmeans_problem_t_fixed} can then be rewritten as
%
\begin{equation}
\begin{split}
 \argmin_{\{ \mathcal{S}_1, \ldots,\mathcal{S}_Q\}, \lambda} & ||\rmA^{(\tau)} - \rmB^{(\tau)}||_{\mathcal{F}} ^ 2  +  \sum_{j=0}^{Q} \delta_j(\mathcal{S}_j)\\
 s.t. &~ \rmB^{(\tau)} = \lambda \prod_{j=0}^{Q}{\mathcal{S}_j}
\end{split}
\end{equation}

Since \textit{Hierarchical PALM4LED} successivly updates the $\mathcal{S}_j$s independently and in an alternating fashion, we can still use \textit{PALM4LED} in to solve this problem with the $\mathcal{S}_0$ fixed.


\begin{algorithm}
\caption{Q-means algorithm}
\label{algo:qmeans}
\begin{algorithmic}[1]


\REQUIRE $\rmX \in \R^{n \times d}$, $K$, $\{\mathcal{S}_1 \dots \mathcal{S}_{Q}\}|\mathcal{S}_j \in \mathcal{E}_j$
\ENSURE $\{\mathcal{S}_1 \dots \mathcal{S}_{Q}\}|\mathcal{S}_j \in \mathcal{E}_j$ such that $\prod_{j=1}^{Q}\mathcal{S}_j \approx \rmU$ the K means of $n$ $d$-dimensional samples
\STATE $\tau \leftarrow 0$
\REPEAT
\STATE $\tau \leftarrow \tau + 1$
\STATE $\rvt^{(\tau)} \leftarrow \argmin_{\rvt \in [\![K]\!]^n} \sum_{i=1}^{n} {||\rmX_i - \rmU^{(\tau -1)}_{\rvt(i)}||_2^2}$
\label{line:qmeans:assignment}
\FORALL {$k \in [\![K]\!]$}
\label{line:qmeans:startkmeans}
\STATE $n_k^{(\tau)} \leftarrow |\{i: \rvt^{(\tau)}_i=k\}|$
\STATE $\hat{\rmX}^{(\tau)}_k \leftarrow \frac{1}{n^\tau_k} \sum_{i: \rvt^\tau_i = k} {\rvx_i}$
\ENDFOR
\label{line:qmeans:endkmeans}
\STATE $\rmA^{(\tau)} \leftarrow \mathcal{D}_{\sqrt{\rvn^{(\tau)}}}~\hat{\rmX}^{(\tau)} $
\STATE $\{\mathcal{S}^{(\tau)}_1 \dots \mathcal{S}^{(\tau)}_{Q}\}, \lambda^{(\tau)} \leftarrow \argmin_{\{\mathcal{S}_1 \dots \mathcal{S}_Q, \lambda\}} ||\rmA^{(\tau)} - ~\lambda\prod_{j=0}^{Q}{\mathcal{S}_j}||_\mathcal{F}^2 + \sum_{j=0}^{Q} \delta_j(\mathcal{S}_j)$
\STATE $\rmU^{(\tau)}_k \leftarrow \lambda^{(\tau)} \prod_{j=1}^{Q}{\mathcal{S}_j^{(\tau)}}$

\UNTIL{stop criterion}
\end{algorithmic}
\end{algorithm}


The factorization of $\rmU$ could then be used in an ulterior algorithm that involves a matrix-vector multiplication with $\rmU$: typically any algorithm involving the assignment of some data points to one of the clusters (Equation \ref{eq:assignment_problem_kmeans}). Such applications of our proposed algorithm are discussed in Section \ref{sec:uses}.

\subsection{Complexity}
\paragraph{Complexity of a multiplication of dense matrices.}
If $\rmA$ is an $M\times K $ matrix and $\rmB$ is an $K \times N$ matrix, then computing $\rmA \rmB$ can be done in $\mathcal{O}\left (MKN\right )$ operations.

\paragraph{Complexity of a multiplication with a sparse matrix.}
If $\rmA$ is a sparse matrix with $a$ non-zero entries and $\rmB$ is a dense matrix with $N$ columns, then computing $\rmA \rmB$ can be done in $\mathcal{O}\left (aN\right )$ operations.
%If $\rmB$ is also sparse, with $b$ non-zero entries, then the bound is $\mathcal{O}\left ( \min\left ( a \min\left (b, N \right ), b \min\left (a, M\right ) \right ) \right )$ where $M$ is the number of rows in $\rmA$.
%This is a naive upper bound for sparse matrices, some tighter bound may be found.

\paragraph{Complexity of algorithm \textit{PALM4LED}.}
Each iteration takes $\mathcal{O}(Q(dpQ + Kd + d^2)$. In the following analysis, we refer to the lines in Algorithm 2 of \cite{magoarou2014learning}. Also, we consider the scenario when the first factor is in $\R^{K \times d}$ and all the others are in $\R^{d \times d}$. Finaly, we set the number of values in each sparse matrix to be the same and equal to $p$.

For each of the $Q$ factors, the complexity is $\mathcal{O}(dpQ + Kd + d^2))$:
%
\begin{itemize}
 \item Lines 3 and 4: $\mathcal{O}(dpQ)$ by computing the products right to left and taking advantage of the sparsity of the factors;
 \item Line 5: $\mathcal{O}(Kd + d^2)$ because $\rmR \in \R^{K \times d}$ and $\rmL \in \R^{d \times d}$;
 \item Line 6: $\mathcal{O}(dpQ + Kd)$ for the sparse product and the projection operation.
\end{itemize}
%
The two last statement of each iteration are in time $\mathcal{O}(dpQ)$, again taking advantage of the sparse factorisation.
%
%\begin{itemize}
% \item Line 8: $\mathcal{O}(dpQ)$;
% \item Line 9: $\mathcal{O}(dpQ)$.
%\end{itemize}

\paragraph{Complexity of algorithm \textit{Hierarchical PALM4LED}.}

The hierarchical version of the algorithm corresponds to the same algorithm repeated $Q$ times. The overall complexity is then $\mathcal{O}(Q^2(dpQ + Kd + d^2))$. We now note that, in practice, $Q$ is supposed to be small compared to $K$ or $d$, and that $p$ should be of the same order than $d$. In that case, we can simplify the complexity of the final algorithm to $\mathcal{O}(Kd + d^2)$.

\paragraph{Complexity of Kmeans (algorithm~\ref{algo:kmeans}).}
Each iteration takes $\mathcal{O}\left (ndk\right )$
\begin{itemize}
\item Assignment, line~\ref{line:kmeans:assignment}: $\mathcal{O}\left (ndk\right )$\\
it is dominated by the computation of $\rmU \rmX^T$ using $\left \|\rvx-\rvu\right \|_2^2=\left \|\rvx\right \|_2^2+\left \|\rvu\right \|_2^2-2\left <\rvx, \rvu\right >$
\item Computing size of cluster, line~\ref{line:kmeans:count}: $\mathcal{O}\left (n \right )$\\
it consists of one pass over $\rvt$.
\item Updating the centroids, line~\ref{line:kmeans:compute_means}: $\mathcal{O}\left (nd\right )$\\
since each example is summed once.
\end{itemize}

\paragraph{Complexity of Q-means (algorithm~\ref{algo:qmeans}).}

Naively, each iteration of the \textit{Q-means} algorithm has the same complexity than K-means with and additional $\mathcal{O}(Kd + d^2)$ for the \textit{Hierarchical PALM4LED} step. This leads to the overall complexity of $\mathcal{O}(Kdn + d^2)$. In the case where the dimensionality of the data is negligible in front of the size of the dataset, this is of the same order of complexity than the vanilla K-means algorithm, e.g. $\mathcal{O}(ndk)$. Nevertheless, we can already take advantage of the decomposition of the center-point matrix in the assignment step of the Algorithm: indeed, when the intermediate center-point matrix $\rmU^{(\tau-1)}$ has the sparse factorization constraint, assigning a $n$ data points to their cluster can be done in time $\mathcal{O}(n K \log d)$ operations, which leads to the new complexity for \textit{Q-means}: $\mathcal{O}(nK\log d + Kd + d^2)$

Once the decomposed center-point matrix has been obtained, the assignation of a new data point to a cluster becomes $\mathcal{O}(nK\log d)$ which would be usefull in many machine learning application usually using the K-means algorithm.
