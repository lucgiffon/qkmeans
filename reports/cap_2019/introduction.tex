\section{Introduction}

Kernel machines are powerful tools to learn non-linear relationships in the data, but they do not scale well to nowadays large, real world, datasets. Indeed, they depend on the kernel matrix which has a very high storage ($\mathcal{O}(n^2)$) and computational ($\mathcal{O}(dn^2)$) cost with respect to the size of the dataset $n$ and the dimensionality of the data $d$.

One plebiscited approach to tackle this problem is to use the Nyström approximation of the kernel matrix. This approach relies on the selection of some landmark points which summarize well the full dataset, and which are used in building a low rank decomposition of the complete kernel matrix. This decomposition is then used as a drop-in replacement for the kernel matrix in further kernel machines such as the SVM. The whole training process in that case is then decomposed in three steps: first, one must select the landmark points; second, the Nyström approximation is built and finally the kernel machine is learned using this approximation. 

The chosen landmark points are crucial for the Nyström approximation. Multiple landmark selection methods have been investigated for their advantages and inconvenients \cite{kumar2012sampling}\cite{musco2017recursive}. One such selection method is the K-means algorithm from which the output center-points are taken for the Nyström landmark points. Although this method has rather high computational cost, it offers consistently one of the best approximation qualities for the Nyström approximation \cite{kumar2012sampling}. Recently, a new approach to landmark point selection has been proposed \cite{si2016computationally}: it aims at rendering the Nyström approximation even more efficient than it is in its vanilla form by taking advantage of some known fast transform algorithm such as the Haar or Hadamard transform. Indeed, they first emphasize that, for a large family of kernel functions, most of the computational cost of the Nyström method lies in the computation of the dot product between the examples of the training or testing set and the landmark points. After that, they propose a procedure to form the matrix of landmark points with some special structure, embedding the Haar or the Hadamard matrix. This special structure allows to use the associated fast transform algorithms when computing the matrix vector product between the landmark point matrix and any point, hence reducing its computational cost from $\mathcal{O}(md)$ to $\mathcal{O}(m)$ when the Haar transform is used, and to $\mathcal{O}(m \log d)$ when it is the Hadamard transform. In this approach, the structure of the landmark point matrix is constrained by the fixed fast-transform matrix. This might be handicaping for further use of the subsequent Nyström approximation.

\textit{Would it be possible to form a landmark point matrix which has both the good approximation properties of the K-means method and the efficiency of fast transforms?} In this paper, we propose an algorithm with convergence proof that could be used to learn the landmark point matrix for the Nyström approximation, that is both close to the K-means center-points matrix and very efficient to use. This algorithm, which we call \textit{Q-means}, is built upon recent work \cite{magoarou2014learning} on approximating a given matrix by a product of few sparse matrices. If the number of sparse matrices involved is small enough, the gain in complexity in the induced matrix-vector multiplication could be of the same order than the one achieved with fast-transform algorithms, while the full reconstructed matrix isn't constrained by some predefined matrix definition.

~\\
In the remaining of this paper, we start in Section \ref{sec:background} by giving some background on the K-means algorithm (Section \ref{sec:kmeans}) and the \textit{Hierarchical PALM4LED} algorithm that allows to learn a fast-transform as the product of sparse matrices (Section \ref{sec:palm4led}). We then describe our algorithm and give its convergence proof in Section \ref{sec:contribution}. Finally we discuss in Section \ref{sec:uses} how our \textit{Q-means} algorithm can be used in the Nyström approximation.

Note that this paper describes work in progress and does not contain any experiments on the subject. In the Conclusion (Section \ref{sec:conclusion}), we discuss foreseen experiments and a larger scope of application of our method.
