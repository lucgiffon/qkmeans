\begin{abstract}
Beyond its popularity for clustering, the K-means algorithm is a pivotal procedure for other core machine learning and data analysis techniques such as indexing, nearest-neightbors prediction, as well as for more specific approaches like the Nyström approximation for kernel machines.

In this paper, we propose {\em Q-means}: an accelerated version of $K$-means that stems from recent advances in optimization to learn the centroid matrix as a product of sparse matrices.
This decomposition provides a structure similar to that of fast transforms (e.g., Fourier, Hadamard) in order to benefit from its computationnal efficiency while being adapted to the training data.
Indeed, the complexity of the matrix-vector product between the factorized $K \times D$ matrix $\mathbf{U}$ and any vector is lowered from $\mathcal{O}(KD)$ to $\mathcal{O}(p+q \log q)$, with $q=\min (K, D)$ and $p=\max (K, D)$.
This dramatic acceleration is beneficial whenever a point is assigned to a cluster, i.e., at prediction time and in the assignation step at learning time.
In addition, we show that the computational overhead due to the decomposition procedure does not penalize the computational cost of the learning stage, 
which may be faster than the traditionnal Lloyd algorithm depending on the context.

Finally, we provide discussions and numerical experiments that show the versatility of the proposed computationally-efficient Q-means algorithm.

\addVE{Remarque: on ne mentionne pas la qualité de l'approximation qu'on obtient en remplaçant K-means par Q-means?!} \addLG{je crois qu'on peut assez peu s'exprimer à ce sujet sans borne...}

\addVE{Remarque sur la complexité $\mathcal{O}(p+q \log q)$: on a $\log q$ facteurs dont un de taille $p\times q$ (ou l'inverse) a $\mathcal{O}(p)$ valeurs non-nulles et tous les autres de taille $q \times q$ ont $\mathcal{O}(q)$ valeurs non-nulles.}


%on core machine learning models and techniques such as Nyström approximation, Gaussian mixtures, K-nearest neighbors.

%As evoked before, we show how our algorithm could be used in the context of the Nyström approximation. % to speed up its use in subsequent kernel machines.

% This paper is a description of work in progress and as such does not contain experimental validation. We give the proposed algorithm with convergence proof, and discuss foreseen experiments and a scope of use.
\end{abstract}
