\begin{abstract}
Beyond its popularity for clustering, the K-means algorithm is a pivotal procedure for other core machine learning and data analysis techniques such as indexing, nearest-neightbors prediction, as well as \addLG{more specific approaches like the Nyström approximation for kernel machines} for more specific approaches like kernel methods accelerated by some Nyström approximation.


We here propose {\em Q-means}, an accelerated
version of $K$-means, that looks for the matrix $\mathbf{U}$ of the $K$ centroids as a product
of sparse matrices. 
This decomposition provides a computationally-efficient structure similar to that of fast transforms (e.g., Fourier, Hadamard).
Recent advances in optimization allows to learn such a decomposition in order benefit both from its computationnal efficiency and from its adaptability to the training data. As a result, computing the matrix-vector product between the $K \times D$ matrix $\mathbf{U}$ and any vector is obtained in time $\mathcal{O}(P \log P)$ instead of $\mathcal{O}(KD)$.

\addLG{In this paper, we propose {\em Q-means}: an accelerated version of $K$-means that stems from recent advances in optimization to learn the centroid matrix as a product of sparse matrices. Such decomposition provides a computationally-efficient structure with  similar performance to that of fast transforms (e.g., Fourier, Hadamard) while being attached to the data. Indeed, the complexity of the matrix-vector product between the factorized $K \times D$ matrix $\mathbf{U}$ and any vector is lowered from $\mathcal{O}(KD)$ to $\mathcal{O}(p \log q)$, with $q=\min (K, D)$ and $p$ the number of non-zero values in each factor of the decomposition}


This dramatic acceleration is beneficial whenever a point is assigned to a cluster, i.e., at prediction time and in the assignation step at learning time.
In addition, we show that the computational overhead due to the decomposition procedure does not penalize the computational cost of the learning stage, 
which may be faster than the traditionnal Lloyd algorithm depending on the context.
Finally, we provide discussions and numerical experiments that show the versatility of the proposed computationally-efficient Q-means algorithm.

Remarque: on ne mentionne pas la qualité de l'approximation qu'on obtient en remplaçant K-means par Q-means?! \addLG{je crois qu'on peut assez peu s'exprimer à ce sujet sans borne...}

%on core machine learning models and techniques such as Nyström approximation, Gaussian mixtures, K-nearest neighbors.

%As evoked before, we show how our algorithm could be used in the context of the Nyström approximation. % to speed up its use in subsequent kernel machines.

% This paper is a description of work in progress and as such does not contain experimental validation. We give the proposed algorithm with convergence proof, and discuss foreseen experiments and a scope of use.
\end{abstract}
