\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Document de travail QALM}
\author{luc.giffon }
\date{March 2019}

\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}

\def\rmGamma{{\mathbf{\Gamma}}}

\input{math_commands.tex}

\begin{document}

\maketitle

\section{Notations}

$[\![K]\!]$ with $K \in \sN$ is the set of all i such as $i \in \sN$ and $i < K$ . $\rmX = \{\rvx_i \in \R^d\}_{i=1}^n$ is the considered dataset.

\section{Preliminaries}

\subsection{K-means}

The K-means algorithm is used to partition a given set of observations $\rmX$ into a predefined amount of $K$ clusters. The algorithm starts with an initialized set of $K$ center-points ($\{\rmU_i \in \R\}_{i=1}^{K}$). Each update step is divided in two parts: (i) all observations $\rvx_i$ are assigned to their nearest cluster based on the center-points $\rmU_i$s (Equation \ref{eq:kmeans_assign_step}) in $O(ndK)$ operations. (ii) the new center-points are computed as the means of the assigned $\rvx_i$ (Equation \ref{eq:kmeans_compute_step}) for a total of $O(nd)$ operations.

\begin{algorithm}
\caption{K-means algorithm}
\begin{algorithmic}


\REQUIRE $\rmX \in \R^{n \times d}$, $K$, $\{\rmU_i \in \R^d\}_{i=1}^{K}$
\ENSURE $\{\rmU_i\}_{i=1}^{K}$ the K means of $n$ $d$-dimensional samples
\REPEAT
\STATE \begin{equation}
\label{eq:kmeans_assign_step}
t \leftarrow \argmin_{t \in [\![K]\!]^n} \sum_{i=1}^{n} {||\rvx_i - \rmU_{t(i)}||_2^2}
\end{equation}
\FORALL {$k \in [\![K]\!]^n$}
\STATE $n_k \leftarrow |\{i: t_i=k\}|$
\STATE \begin{equation}
\label{eq:kmeans_compute_step}
\rmU_k \leftarrow \frac{1}{n_k} \sum_{i: t_i = k} {\rvx_i}
\end{equation}
\ENDFOR
\UNTIL{$i=0$}
\end{algorithmic}
\end{algorithm}


Once the cluster have been defined, for any $\rvx \in \R^d$ the cluster associated with this $\rvx$ is:

\begin{equation}
\argmin_{t \in [\![K]\!]^n} \sum_{i=1}^{n} {||\rvx - \rmU_{t(i)}||_2^2}
\end{equation}


\subsection{Learning Fast transforms as the product of sparse matrices}

A popular way for providing concise description of high-dimensional vectors $\rmU \in \R^{K \times d}$ is to compute a sparse representation using a dictionnary:

\begin{equation}
\rmU^T \approx \rmD\rmGamma
\end{equation}

where $\rmD \in \R^{d \times d}$ is a dictionnary and $\rmGamma \in \R^{d \times K}$ has sparse columns. Historically, the dictionnary is chosen either to allows for (i) a fast reconstruction of the initial matrix in which case one would choose $\rmD$ so that we can use a known fast-transform algorithm such as the \textit{Fast Hadamard Transform} or (ii) a good reconstruction of the initial matrix in which case $\rmD$ is learned from the data itself.

Building on the observation that a fast-transform associated with a "fast" dictionnary can be expressed as the product of sparse matrices, \cite{magoarou} proposes an algorithm to learn a dictionnary from the data with sparsity constraints such that this dictionnary would be both well-suited with the data and fast to use:

\begin{equation}
\rmD = \prod_{j=1}^{M}\rmS_j
\end{equation}

with $\rmS_j \in \mathcal{E}_j$, $\mathcal{E}_j = \lbrace \rmA \in \R^{a \times a+1}~\text{s.t.}~||\rmA||_0^0 \leq p_j \rbrace$ and $p_j$ being chosen suitably.

Considering $\rmGamma$ being a sparse matrice too, it can be renamed as $\rmGamma = \rmS_{M+1}$ and the overall dictionnary learning problem can be simplified to finding the sparse matrix product approximation:

\begin{equation}
\rmU \approx \prod_{j=1}^{M+1}\rmS_j
\end{equation}

which lead to the following optimisation problem

\begin{equation}
\min_{\{\rmS_1 \dots \rmS_P, \lambda\}} ||\rmU - \lambda \prod_{j=1}^{M+1}{\rmS_j}||_2^2 + \sum_{j=1}^{M+1} \delta_j(\rmS_j)
\end{equation}

with the $\delta_j(\rmS_j) = 0$ if $\rmS_j \in \mathcal{E}_j$ being some constraints to satisfy on the associated $\rmS_j$.

This problem is highly non-convex but a good local minima can be found using the PALM algorithm.


\section{Contribution}

\subsection{Q-means}

We propose an extension of the K-means algorithm so that the output matrix of center-points $\rmU$ would be approximated by a product of $M+1$ sparse matrices $\rmS_j$. The factorization of $\rmU$ could then be used in ulterior algorithm that involve a matrix-vector multiplication with $\rmU$. Such applications of our proposed algorithm are discussed in Section \ref{sec:uses}.

\begin{algorithm}
\caption{Q-means algorithm}
\begin{algorithmic}


\REQUIRE $\rmX \in \R^{n \times d}$, $K$, $\{\rmU_i \in \R^d\}_{i=1}^{K}$, $\{\rmS_1 \dots \rmS_{M+1}\}|\rmS_j \in \mathcal{E}_j$
\ENSURE $\{\rmS_1 \dots \rmS_{M+1}\}|\rmS_j \in \mathcal{E}_j$ such that $\prod_{j=1}^{M+1}\rmS_j \approx \rmU$ the K means of $n$ $d$-dimensional samples
\REPEAT
\STATE \begin{equation}
\label{eq:kmeans_assign_step}
t \leftarrow \argmin_{t \in [\![K]\!]^n} \sum_{i=1}^{n} {||\rvx_i - \rmU_{t(i)}||_2^2}
\end{equation}
\FORALL {$k \in [\![K]\!]^n$}
\STATE $n_k \leftarrow |\{i: t_i=k\}|$
\STATE \begin{equation}
\label{eq:kmeans_compute_step}
\rmU_k \leftarrow \frac{1}{n_k} \sum_{i: t_i = k} {\rvx_i}
\end{equation}
\STATE $\{\rmS_1 \dots \rmS_{M+1}\} \leftarrow \argmin_{\{\rmS_1 \dots \rmS_P, \lambda\}} ||\rmU - \lambda \prod_{j=1}^{M+1}{\rmS_j}||_2^2 + \sum_{j=1}^{M+1} \delta_j(\rmS_j)$
\ENDFOR
\UNTIL{$i=0$}
\end{algorithmic}
\end{algorithm}


\section{Utilisation}
\label{sec:uses}

\subsection{Nyström}

\subsubsection{Efficient Nyström}

Explication du papier efficient Nyström

\subsubsection{Qalm in Nyström}

Construction remplacement d'une matrice de hadamard par une transformée apprise

\subsection{RBF networks}

Besoin d'éclaircir les liens avec RBF networks

%\bibliographystyle{plain}
%\bibliography{references}
\end{document}
